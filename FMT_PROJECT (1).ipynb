{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9beea57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from numpy import percentile\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bb490",
   "metadata": {},
   "source": [
    "1. Import and understand the data.\n",
    "   A. Import ‘signal-data.csv’ as DataFrame. \n",
    "   B. Print 5 point summary and share at least 2 observations. ["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a59942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-07-19 11:55:00</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-07-19 12:32:00</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>...</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-07-19 13:17:00</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>...</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-07-19 14:43:00</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>...</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-07-19 15:22:00</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 592 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Time        0        1          2          3       4      5  \\\n",
       "0  2008-07-19 11:55:00  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   \n",
       "1  2008-07-19 12:32:00  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0   \n",
       "2  2008-07-19 13:17:00  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   \n",
       "3  2008-07-19 14:43:00  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0   \n",
       "4  2008-07-19 15:22:00  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0   \n",
       "\n",
       "          6       7       8  ...       581     582     583     584      585  \\\n",
       "0   97.6133  0.1242  1.5005  ...       NaN  0.5005  0.0118  0.0035   2.3630   \n",
       "1  102.3433  0.1247  1.4966  ...  208.2045  0.5019  0.0223  0.0055   4.4447   \n",
       "2   95.4878  0.1241  1.4436  ...   82.8602  0.4958  0.0157  0.0039   3.1745   \n",
       "3  104.2367  0.1217  1.4882  ...   73.8432  0.4990  0.0103  0.0025   2.0544   \n",
       "4  100.3967  0.1235  1.5031  ...       NaN  0.4800  0.4766  0.1045  99.3032   \n",
       "\n",
       "      586     587     588       589  Pass/Fail  \n",
       "0     NaN     NaN     NaN       NaN         -1  \n",
       "1  0.0096  0.0201  0.0060  208.2045         -1  \n",
       "2  0.0584  0.0484  0.0148   82.8602          1  \n",
       "3  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "4  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "\n",
       "[5 rows x 592 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_data = pd.read_csv('signal-data.csv')\n",
    "signal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943e5e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time          object\n",
       "0            float64\n",
       "1            float64\n",
       "2            float64\n",
       "3            float64\n",
       "              ...   \n",
       "586          float64\n",
       "587          float64\n",
       "588          float64\n",
       "589          float64\n",
       "Pass/Fail      int64\n",
       "Length: 592, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4ebeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3030.93\n",
       "1       3095.78\n",
       "2       2932.61\n",
       "3       2988.72\n",
       "4       3032.24\n",
       "         ...   \n",
       "1562    2899.41\n",
       "1563    3052.31\n",
       "1564    2978.81\n",
       "1565    2894.92\n",
       "1566    2944.92\n",
       "Name: 0, Length: 1567, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edb6af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1561.000000</td>\n",
       "      <td>1560.000000</td>\n",
       "      <td>1553.000000</td>\n",
       "      <td>1553.000000</td>\n",
       "      <td>1553.000000</td>\n",
       "      <td>1553.0</td>\n",
       "      <td>1553.000000</td>\n",
       "      <td>1558.000000</td>\n",
       "      <td>1565.000000</td>\n",
       "      <td>1565.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>618.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1566.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3014.452896</td>\n",
       "      <td>2495.850231</td>\n",
       "      <td>2200.547318</td>\n",
       "      <td>1396.376627</td>\n",
       "      <td>4.197013</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.112908</td>\n",
       "      <td>0.121822</td>\n",
       "      <td>1.462862</td>\n",
       "      <td>-0.000841</td>\n",
       "      <td>...</td>\n",
       "      <td>97.934373</td>\n",
       "      <td>0.500096</td>\n",
       "      <td>0.015318</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>3.067826</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>99.670066</td>\n",
       "      <td>-0.867262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>73.621787</td>\n",
       "      <td>80.407705</td>\n",
       "      <td>29.513152</td>\n",
       "      <td>441.691640</td>\n",
       "      <td>56.355540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.237214</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.073897</td>\n",
       "      <td>0.015116</td>\n",
       "      <td>...</td>\n",
       "      <td>87.520966</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.017180</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>3.578033</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>93.891919</td>\n",
       "      <td>0.498010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2743.240000</td>\n",
       "      <td>2158.750000</td>\n",
       "      <td>2060.660000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>82.131100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>-0.053400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477800</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.197500</td>\n",
       "      <td>-0.016900</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2966.260000</td>\n",
       "      <td>2452.247500</td>\n",
       "      <td>2181.044400</td>\n",
       "      <td>1081.875800</td>\n",
       "      <td>1.017700</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.920000</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>1.411200</td>\n",
       "      <td>-0.010800</td>\n",
       "      <td>...</td>\n",
       "      <td>46.184900</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>2.306500</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>44.368600</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3011.490000</td>\n",
       "      <td>2499.405000</td>\n",
       "      <td>2201.066700</td>\n",
       "      <td>1285.214400</td>\n",
       "      <td>1.316800</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.512200</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>1.461600</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>...</td>\n",
       "      <td>72.288900</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>2.757650</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>71.900500</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3056.650000</td>\n",
       "      <td>2538.822500</td>\n",
       "      <td>2218.055500</td>\n",
       "      <td>1591.223500</td>\n",
       "      <td>1.525700</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.586700</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>1.516900</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>...</td>\n",
       "      <td>116.539150</td>\n",
       "      <td>0.502375</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>3.295175</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>114.749700</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3356.350000</td>\n",
       "      <td>2846.440000</td>\n",
       "      <td>2315.266700</td>\n",
       "      <td>3715.041700</td>\n",
       "      <td>1114.536600</td>\n",
       "      <td>100.0</td>\n",
       "      <td>129.252200</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>1.656400</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>...</td>\n",
       "      <td>737.304800</td>\n",
       "      <td>0.509800</td>\n",
       "      <td>0.476600</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>99.303200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>737.304800</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2            3            4  \\\n",
       "count  1561.000000  1560.000000  1553.000000  1553.000000  1553.000000   \n",
       "mean   3014.452896  2495.850231  2200.547318  1396.376627     4.197013   \n",
       "std      73.621787    80.407705    29.513152   441.691640    56.355540   \n",
       "min    2743.240000  2158.750000  2060.660000     0.000000     0.681500   \n",
       "25%    2966.260000  2452.247500  2181.044400  1081.875800     1.017700   \n",
       "50%    3011.490000  2499.405000  2201.066700  1285.214400     1.316800   \n",
       "75%    3056.650000  2538.822500  2218.055500  1591.223500     1.525700   \n",
       "max    3356.350000  2846.440000  2315.266700  3715.041700  1114.536600   \n",
       "\n",
       "            5            6            7            8            9  ...  \\\n",
       "count  1553.0  1553.000000  1558.000000  1565.000000  1565.000000  ...   \n",
       "mean    100.0   101.112908     0.121822     1.462862    -0.000841  ...   \n",
       "std       0.0     6.237214     0.008961     0.073897     0.015116  ...   \n",
       "min     100.0    82.131100     0.000000     1.191000    -0.053400  ...   \n",
       "25%     100.0    97.920000     0.121100     1.411200    -0.010800  ...   \n",
       "50%     100.0   101.512200     0.122400     1.461600    -0.001300  ...   \n",
       "75%     100.0   104.586700     0.123800     1.516900     0.008400  ...   \n",
       "max     100.0   129.252200     0.128600     1.656400     0.074900  ...   \n",
       "\n",
       "              581          582          583          584          585  \\\n",
       "count  618.000000  1566.000000  1566.000000  1566.000000  1566.000000   \n",
       "mean    97.934373     0.500096     0.015318     0.003847     3.067826   \n",
       "std     87.520966     0.003404     0.017180     0.003720     3.578033   \n",
       "min      0.000000     0.477800     0.006000     0.001700     1.197500   \n",
       "25%     46.184900     0.497900     0.011600     0.003100     2.306500   \n",
       "50%     72.288900     0.500200     0.013800     0.003600     2.757650   \n",
       "75%    116.539150     0.502375     0.016500     0.004100     3.295175   \n",
       "max    737.304800     0.509800     0.476600     0.104500    99.303200   \n",
       "\n",
       "               586          587          588          589    Pass/Fail  \n",
       "count  1566.000000  1566.000000  1566.000000  1566.000000  1567.000000  \n",
       "mean      0.021458     0.016475     0.005283    99.670066    -0.867262  \n",
       "std       0.012358     0.008808     0.002867    93.891919     0.498010  \n",
       "min      -0.016900     0.003200     0.001000     0.000000    -1.000000  \n",
       "25%       0.013425     0.010600     0.003300    44.368600    -1.000000  \n",
       "50%       0.020500     0.014800     0.004600    71.900500    -1.000000  \n",
       "75%       0.027600     0.020300     0.006400   114.749700    -1.000000  \n",
       "max       0.102800     0.079900     0.028600   737.304800     1.000000  \n",
       "\n",
       "[8 rows x 591 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1afc36",
   "metadata": {},
   "source": [
    "a)There are outliers in certain columns. \n",
    "b)Some columns have Nan values. \n",
    "d)Certain column (eg. col 5. have all its entries to be100.)\n",
    "d)The first column Time need to be converted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f29c5",
   "metadata": {},
   "source": [
    "2. Data cleansing:\n",
    "A. Write a for loop which will remove all the features with 20%+ Null values and impute rest with mean of the feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7bb1246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.771537970644545"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_percent=signal_data.isnull().sum().sum()/len(signal_data)\n",
    "null_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e0a99c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008934269304403318"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_percent_1 = signal_data.iloc[:,3].isnull().sum().sum()/len(signal_data)\n",
    "null_percent_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96b21238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time         datetime64[ns]\n",
       "0                   float64\n",
       "1                   float64\n",
       "2                   float64\n",
       "3                   float64\n",
       "                  ...      \n",
       "586                 float64\n",
       "587                 float64\n",
       "588                 float64\n",
       "589                 float64\n",
       "Pass/Fail             int64\n",
       "Length: 592, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_data['Time'] = pd.to_datetime(signal_data['Time'])\n",
    "signal_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9adffb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time          0\n",
       "0             6\n",
       "1             7\n",
       "2            14\n",
       "3            14\n",
       "             ..\n",
       "586           1\n",
       "587           1\n",
       "588           1\n",
       "589           1\n",
       "Pass/Fail     0\n",
       "Length: 592, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cf882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dataframe:\n",
      "                    Time        0        1          2          3       4  \\\n",
      "0    2008-07-19 11:55:00  3030.93  2564.00  2187.7333  1411.1265  1.3602   \n",
      "1    2008-07-19 12:32:00  3095.78  2465.14  2230.4222  1463.6606  0.8294   \n",
      "2    2008-07-19 13:17:00  2932.61  2559.94  2186.4111  1698.0172  1.5102   \n",
      "3    2008-07-19 14:43:00  2988.72  2479.90  2199.0333   909.7926  1.3204   \n",
      "4    2008-07-19 15:22:00  3032.24  2502.87  2233.3667  1326.5200  1.5334   \n",
      "...                  ...      ...      ...        ...        ...     ...   \n",
      "1562 2008-10-16 15:13:00  2899.41  2464.36  2179.7333  3085.3781  1.4843   \n",
      "1563 2008-10-16 20:49:00  3052.31  2522.55  2198.5667  1124.6595  0.8763   \n",
      "1564 2008-10-17 05:26:00  2978.81  2379.78  2206.3000  1110.4967  0.8236   \n",
      "1565 2008-10-17 06:01:00  2894.92  2532.01  2177.0333  1183.7287  1.5726   \n",
      "1566 2008-10-17 06:07:00  2944.92  2450.76  2195.4444  2914.1792  1.5978   \n",
      "\n",
      "          5         6       7         8  ...      577     582     583     584  \\\n",
      "0     100.0   97.6133  0.1242  1.500500  ...  14.9509  0.5005  0.0118  0.0035   \n",
      "1     100.0  102.3433  0.1247  1.496600  ...  10.9003  0.5019  0.0223  0.0055   \n",
      "2     100.0   95.4878  0.1241  1.443600  ...   9.2721  0.4958  0.0157  0.0039   \n",
      "3     100.0  104.2367  0.1217  1.488200  ...   8.5831  0.4990  0.0103  0.0025   \n",
      "4     100.0  100.3967  0.1235  1.503100  ...  10.9698  0.4800  0.4766  0.1045   \n",
      "...     ...       ...     ...       ...  ...      ...     ...     ...     ...   \n",
      "1562  100.0   82.2467  0.1248  1.342400  ...  11.7256  0.4988  0.0143  0.0039   \n",
      "1563  100.0   98.4689  0.1205  1.433300  ...  17.8379  0.4975  0.0131  0.0036   \n",
      "1564  100.0   99.4122  0.1208  1.462862  ...  17.7267  0.4987  0.0153  0.0041   \n",
      "1565  100.0   98.7978  0.1213  1.462200  ...  19.2104  0.5004  0.0178  0.0038   \n",
      "1566  100.0   85.1011  0.1235  1.462862  ...  22.9183  0.4987  0.0181  0.0040   \n",
      "\n",
      "          585       586       587       588         589  Pass/Fail  \n",
      "0      2.3630  0.021458  0.016475  0.005283   99.670066         -1  \n",
      "1      4.4447  0.009600  0.020100  0.006000  208.204500         -1  \n",
      "2      3.1745  0.058400  0.048400  0.014800   82.860200          1  \n",
      "3      2.0544  0.020200  0.014900  0.004400   73.843200         -1  \n",
      "4     99.3032  0.020200  0.014900  0.004400   73.843200         -1  \n",
      "...       ...       ...       ...       ...         ...        ...  \n",
      "1562   2.8669  0.006800  0.013800  0.004700  203.172000         -1  \n",
      "1563   2.6238  0.006800  0.013800  0.004700  203.172000         -1  \n",
      "1564   3.0590  0.019700  0.008600  0.002500   43.523100         -1  \n",
      "1565   3.5662  0.026200  0.024500  0.007500   93.494100         -1  \n",
      "1566   3.6275  0.011700  0.016200  0.004500  137.784400         -1  \n",
      "\n",
      "[1567 rows x 560 columns]\n"
     ]
    }
   ],
   "source": [
    "#remove all the features with 20%+ Null values and impute rest with mean of the feature.\n",
    "temp1 = signal_data\n",
    "for col in signal_data.columns:\n",
    "    null_percent_1 = temp1[col].isnull().sum().sum()/len(temp1)\n",
    "    if null_percent_1 > 0.2:\n",
    "        del signal_data[col]\n",
    "    else :\n",
    "        mean_value=signal_data[col].mean()\n",
    "        signal_data[col].fillna(value=mean_value, inplace=True)\n",
    "print('Updated Dataframe:')\n",
    "print(signal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6a3a8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time         0\n",
       "0            0\n",
       "1            0\n",
       "2            0\n",
       "3            0\n",
       "            ..\n",
       "586          0\n",
       "587          0\n",
       "588          0\n",
       "589          0\n",
       "Pass/Fail    0\n",
       "Length: 560, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae44cf84",
   "metadata": {},
   "source": [
    "B. Identify and drop the features which are having same value for all the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2d3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    " signal_data = signal_data.loc[:, signal_data.nunique() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abfa9cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>577</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-07-19 11:55:00</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>99.670066</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-07-19 12:32:00</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>208.204500</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-07-19 13:17:00</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>82.860200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-07-19 14:43:00</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>73.843200</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-07-19 15:22:00</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>73.843200</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008-07-19 17:53:00</td>\n",
       "      <td>2946.25</td>\n",
       "      <td>2432.84</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5287</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>...</td>\n",
       "      <td>13.7755</td>\n",
       "      <td>0.4949</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>3.8276</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>44.007700</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008-07-19 19:44:00</td>\n",
       "      <td>3030.27</td>\n",
       "      <td>2430.12</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.5816</td>\n",
       "      <td>-0.0270</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3645</td>\n",
       "      <td>0.5010</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>2.8515</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>44.007700</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008-07-19 19:45:00</td>\n",
       "      <td>3058.88</td>\n",
       "      <td>2690.15</td>\n",
       "      <td>2248.9000</td>\n",
       "      <td>1004.4692</td>\n",
       "      <td>0.7884</td>\n",
       "      <td>106.2400</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>1.5153</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0862</td>\n",
       "      <td>0.4984</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>2.1261</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>95.031000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008-07-19 20:24:00</td>\n",
       "      <td>2967.68</td>\n",
       "      <td>2600.47</td>\n",
       "      <td>2248.9000</td>\n",
       "      <td>1004.4692</td>\n",
       "      <td>0.7884</td>\n",
       "      <td>106.2400</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>1.5358</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>...</td>\n",
       "      <td>14.2892</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>3.4456</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>111.652500</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008-07-19 21:35:00</td>\n",
       "      <td>3016.11</td>\n",
       "      <td>2428.37</td>\n",
       "      <td>2248.9000</td>\n",
       "      <td>1004.4692</td>\n",
       "      <td>0.7884</td>\n",
       "      <td>106.2400</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>1.5381</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>7.4181</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.0687</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>90.229400</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Time        0        1          2          3       4  \\\n",
       "0 2008-07-19 11:55:00  3030.93  2564.00  2187.7333  1411.1265  1.3602   \n",
       "1 2008-07-19 12:32:00  3095.78  2465.14  2230.4222  1463.6606  0.8294   \n",
       "2 2008-07-19 13:17:00  2932.61  2559.94  2186.4111  1698.0172  1.5102   \n",
       "3 2008-07-19 14:43:00  2988.72  2479.90  2199.0333   909.7926  1.3204   \n",
       "4 2008-07-19 15:22:00  3032.24  2502.87  2233.3667  1326.5200  1.5334   \n",
       "5 2008-07-19 17:53:00  2946.25  2432.84  2233.3667  1326.5200  1.5334   \n",
       "6 2008-07-19 19:44:00  3030.27  2430.12  2230.4222  1463.6606  0.8294   \n",
       "7 2008-07-19 19:45:00  3058.88  2690.15  2248.9000  1004.4692  0.7884   \n",
       "8 2008-07-19 20:24:00  2967.68  2600.47  2248.9000  1004.4692  0.7884   \n",
       "9 2008-07-19 21:35:00  3016.11  2428.37  2248.9000  1004.4692  0.7884   \n",
       "\n",
       "          6       7       8       9  ...      577     582     583     584  \\\n",
       "0   97.6133  0.1242  1.5005  0.0162  ...  14.9509  0.5005  0.0118  0.0035   \n",
       "1  102.3433  0.1247  1.4966 -0.0005  ...  10.9003  0.5019  0.0223  0.0055   \n",
       "2   95.4878  0.1241  1.4436  0.0041  ...   9.2721  0.4958  0.0157  0.0039   \n",
       "3  104.2367  0.1217  1.4882 -0.0124  ...   8.5831  0.4990  0.0103  0.0025   \n",
       "4  100.3967  0.1235  1.5031 -0.0031  ...  10.9698  0.4800  0.4766  0.1045   \n",
       "5  100.3967  0.1235  1.5287  0.0167  ...  13.7755  0.4949  0.0189  0.0044   \n",
       "6  102.3433  0.1247  1.5816 -0.0270  ...   8.3645  0.5010  0.0143  0.0042   \n",
       "7  106.2400  0.1185  1.5153  0.0157  ...  16.0862  0.4984  0.0106  0.0034   \n",
       "8  106.2400  0.1185  1.5358  0.0111  ...  14.2892  0.4993  0.0172  0.0046   \n",
       "9  106.2400  0.1185  1.5381  0.0159  ...   7.4181  0.4967  0.0152  0.0038   \n",
       "\n",
       "       585       586       587       588         589  Pass/Fail  \n",
       "0   2.3630  0.021458  0.016475  0.005283   99.670066         -1  \n",
       "1   4.4447  0.009600  0.020100  0.006000  208.204500         -1  \n",
       "2   3.1745  0.058400  0.048400  0.014800   82.860200          1  \n",
       "3   2.0544  0.020200  0.014900  0.004400   73.843200         -1  \n",
       "4  99.3032  0.020200  0.014900  0.004400   73.843200         -1  \n",
       "5   3.8276  0.034200  0.015100  0.005200   44.007700         -1  \n",
       "6   2.8515  0.034200  0.015100  0.005200   44.007700         -1  \n",
       "7   2.1261  0.020400  0.019400  0.006300   95.031000         -1  \n",
       "8   3.4456  0.011100  0.012400  0.004500  111.652500         -1  \n",
       "9   3.0687  0.021200  0.019100  0.007300   90.229400         -1  \n",
       "\n",
       "[10 rows x 444 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc27821",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = signal_data.drop(['Time'], axis = 1, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e265d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>577</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>99.670066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0148</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>208.204500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 443 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1          2          3       4         6       7       8  \\\n",
       "0  3030.93  2564.00  2187.7333  1411.1265  1.3602   97.6133  0.1242  1.5005   \n",
       "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  102.3433  0.1247  1.4966   \n",
       "\n",
       "        9      10  ...      577     582     583     584     585       586  \\\n",
       "0  0.0162 -0.0034  ...  14.9509  0.5005  0.0118  0.0035  2.3630  0.021458   \n",
       "1 -0.0005 -0.0148  ...  10.9003  0.5019  0.0223  0.0055  4.4447  0.009600   \n",
       "\n",
       "        587       588         589  Pass/Fail  \n",
       "0  0.016475  0.005283   99.670066          1  \n",
       "1  0.020100  0.006000  208.204500          1  \n",
       "\n",
       "[2 rows x 443 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting Pass as 1 and Fail as 0\n",
    "signal_data['Pass/Fail'] = signal_data['Pass/Fail'].replace(to_replace = [-1,1], value = [1,0])\n",
    "signal_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d9bdc",
   "metadata": {},
   "source": [
    "2C. Drop other features if required using relevant functional knowledge. Clearly justify the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a50487",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = signal_data.drop(['Pass/Fail'], axis = 1, inplace = False)\n",
    "y = signal_data['Pass/Fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb5af1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.03093000e+03, 2.56400000e+03, 2.18773330e+03, ...,\n",
       "        1.64749042e-02, 5.28333333e-03, 9.96700663e+01],\n",
       "       [3.09578000e+03, 2.46514000e+03, 2.23042220e+03, ...,\n",
       "        2.01000000e-02, 6.00000000e-03, 2.08204500e+02],\n",
       "       [2.93261000e+03, 2.55994000e+03, 2.18641110e+03, ...,\n",
       "        4.84000000e-02, 1.48000000e-02, 8.28602000e+01],\n",
       "       ...,\n",
       "       [2.97881000e+03, 2.37978000e+03, 2.20630000e+03, ...,\n",
       "        8.60000000e-03, 2.50000000e-03, 4.35231000e+01],\n",
       "       [2.89492000e+03, 2.53201000e+03, 2.17703330e+03, ...,\n",
       "        2.45000000e-02, 7.50000000e-03, 9.34941000e+01],\n",
       "       [2.94492000e+03, 2.45076000e+03, 2.19544440e+03, ...,\n",
       "        1.62000000e-02, 4.50000000e-03, 1.37784400e+02]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying Select Kbest to extract  features based on scores\n",
    "sel_fs = SelectKBest(score_func = f_classif, k= 'all')\n",
    "sel_fs.fit(X,y)\n",
    "# transform input data\n",
    "X_fs = sel_fs.transform(X)\n",
    "X_df = pd.DataFrame(X_fs)\n",
    "#X_df\n",
    "X_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32d80f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.989819\n",
      "Feature 1: 0.010602\n",
      "Feature 2: 0.001434\n",
      "Feature 3: 0.949412\n",
      "Feature 4: 0.296189\n",
      "Feature 5: 0.412804\n",
      "Feature 6: 0.264149\n",
      "Feature 7: 1.229361\n",
      "Feature 8: 1.523998\n",
      "Feature 9: 1.772982\n",
      "Feature 10: 1.667060\n",
      "Feature 11: 0.055760\n",
      "Feature 12: 7.481229\n",
      "Feature 13: 0.013021\n",
      "Feature 14: 0.008688\n",
      "Feature 15: 0.147179\n",
      "Feature 16: 0.122662\n",
      "Feature 17: 1.458414\n",
      "Feature 18: 0.846650\n",
      "Feature 19: 18.468623\n",
      "Feature 20: 8.472554\n",
      "Feature 21: 0.259399\n",
      "Feature 22: 0.524115\n",
      "Feature 23: 2.073962\n",
      "Feature 24: 10.373290\n",
      "Feature 25: 1.336053\n",
      "Feature 26: 18.045409\n",
      "Feature 27: 0.458572\n",
      "Feature 28: 2.170141\n",
      "Feature 29: 3.767981\n",
      "Feature 30: 4.264878\n",
      "Feature 31: 10.321574\n",
      "Feature 32: 0.010065\n",
      "Feature 33: 0.236044\n",
      "Feature 34: 0.010065\n",
      "Feature 35: 2.677516\n",
      "Feature 36: 4.342067\n",
      "Feature 37: 0.007079\n",
      "Feature 38: 4.204600\n",
      "Feature 39: 0.009628\n",
      "Feature 40: 0.656022\n",
      "Feature 41: 0.841267\n",
      "Feature 42: 0.109051\n",
      "Feature 43: 0.138679\n",
      "Feature 44: 1.242320\n",
      "Feature 45: 0.364850\n",
      "Feature 46: 0.360477\n",
      "Feature 47: 0.056431\n",
      "Feature 48: 0.070832\n",
      "Feature 49: 0.309288\n",
      "Feature 50: 0.125306\n",
      "Feature 51: 6.371993\n",
      "Feature 52: 0.596663\n",
      "Feature 53: 4.855748\n",
      "Feature 54: 38.918466\n",
      "Feature 55: 0.526715\n",
      "Feature 56: 0.242226\n",
      "Feature 57: 0.565909\n",
      "Feature 58: 5.637715\n",
      "Feature 59: 9.225108\n",
      "Feature 60: 4.716904\n",
      "Feature 61: 2.387219\n",
      "Feature 62: 2.449617\n",
      "Feature 63: 4.256041\n",
      "Feature 64: 4.245332\n",
      "Feature 65: 0.006512\n",
      "Feature 66: 0.071592\n",
      "Feature 67: 1.222786\n",
      "Feature 68: 4.866008\n",
      "Feature 69: 0.045864\n",
      "Feature 70: 2.964662\n",
      "Feature 71: 9.388533\n",
      "Feature 72: 1.503481\n",
      "Feature 73: 0.872272\n",
      "Feature 74: 0.262917\n",
      "Feature 75: 0.939517\n",
      "Feature 76: 0.112326\n",
      "Feature 77: 0.976738\n",
      "Feature 78: 1.449725\n",
      "Feature 79: 1.130289\n",
      "Feature 80: 1.454869\n",
      "Feature 81: 4.488272\n",
      "Feature 82: 0.136983\n",
      "Feature 83: 0.107143\n",
      "Feature 84: 1.518112\n",
      "Feature 85: 0.009687\n",
      "Feature 86: 5.744254\n",
      "Feature 87: 0.143425\n",
      "Feature 88: 0.670534\n",
      "Feature 89: 3.382773\n",
      "Feature 90: 6.666417\n",
      "Feature 91: 0.977973\n",
      "Feature 92: 2.929623\n",
      "Feature 93: 36.613376\n",
      "Feature 94: 2.365234\n",
      "Feature 95: 0.000339\n",
      "Feature 96: 1.300688\n",
      "Feature 97: 0.363853\n",
      "Feature 98: 0.000778\n",
      "Feature 99: 0.002762\n",
      "Feature 100: 7.411519\n",
      "Feature 101: 2.988006\n",
      "Feature 102: 0.260943\n",
      "Feature 103: 0.437626\n",
      "Feature 104: 0.819435\n",
      "Feature 105: 0.049291\n",
      "Feature 106: 0.643725\n",
      "Feature 107: 11.385747\n",
      "Feature 108: 9.669511\n",
      "Feature 109: 8.226210\n",
      "Feature 110: 13.943916\n",
      "Feature 111: 11.087475\n",
      "Feature 112: 4.760336\n",
      "Feature 113: 8.868912\n",
      "Feature 114: 1.611166\n",
      "Feature 115: 16.897044\n",
      "Feature 116: 9.298802\n",
      "Feature 117: 0.505999\n",
      "Feature 118: 0.000209\n",
      "Feature 119: 7.225001\n",
      "Feature 120: 1.462233\n",
      "Feature 121: 1.176168\n",
      "Feature 122: 0.199787\n",
      "Feature 123: 1.383630\n",
      "Feature 124: 3.546649\n",
      "Feature 125: 0.035771\n",
      "Feature 126: 0.290104\n",
      "Feature 127: 0.372525\n",
      "Feature 128: 3.006189\n",
      "Feature 129: 2.305033\n",
      "Feature 130: 3.024341\n",
      "Feature 131: 2.967679\n",
      "Feature 132: 0.101117\n",
      "Feature 133: 0.049139\n",
      "Feature 134: 0.230323\n",
      "Feature 135: 0.786668\n",
      "Feature 136: 0.033259\n",
      "Feature 137: 0.738076\n",
      "Feature 138: 0.003830\n",
      "Feature 139: 1.085976\n",
      "Feature 140: 0.007258\n",
      "Feature 141: 9.548333\n",
      "Feature 142: 12.691243\n",
      "Feature 143: 0.595891\n",
      "Feature 144: 0.052730\n",
      "Feature 145: 15.768556\n",
      "Feature 146: 12.115639\n",
      "Feature 147: 11.085456\n",
      "Feature 148: 4.263658\n",
      "Feature 149: 0.783054\n",
      "Feature 150: 0.085395\n",
      "Feature 151: 0.203636\n",
      "Feature 152: 2.103152\n",
      "Feature 153: 0.018123\n",
      "Feature 154: 2.165299\n",
      "Feature 155: 0.083977\n",
      "Feature 156: 2.167472\n",
      "Feature 157: 2.884626\n",
      "Feature 158: 0.320744\n",
      "Feature 159: 0.020592\n",
      "Feature 160: 9.740457\n",
      "Feature 161: 2.944991\n",
      "Feature 162: 1.531675\n",
      "Feature 163: 4.853216\n",
      "Feature 164: 0.015870\n",
      "Feature 165: 0.126840\n",
      "Feature 166: 0.050928\n",
      "Feature 167: 3.489204\n",
      "Feature 168: 2.142319\n",
      "Feature 169: 4.471451\n",
      "Feature 170: 4.917483\n",
      "Feature 171: 2.463687\n",
      "Feature 172: 4.240144\n",
      "Feature 173: 5.767496\n",
      "Feature 174: 1.299594\n",
      "Feature 175: 0.282130\n",
      "Feature 176: 3.243628\n",
      "Feature 177: 1.439918\n",
      "Feature 178: 4.516686\n",
      "Feature 179: 0.071592\n",
      "Feature 180: 1.959995\n",
      "Feature 181: 0.811316\n",
      "Feature 182: 0.071592\n",
      "Feature 183: 16.613062\n",
      "Feature 184: 3.039433\n",
      "Feature 185: 0.009728\n",
      "Feature 186: 3.034017\n",
      "Feature 187: 1.683140\n",
      "Feature 188: 0.534226\n",
      "Feature 189: 0.423178\n",
      "Feature 190: 0.209754\n",
      "Feature 191: 0.343503\n",
      "Feature 192: 0.231838\n",
      "Feature 193: 0.731109\n",
      "Feature 194: 1.534168\n",
      "Feature 195: 0.001786\n",
      "Feature 196: 0.324661\n",
      "Feature 197: 0.622864\n",
      "Feature 198: 2.726494\n",
      "Feature 199: 0.191220\n",
      "Feature 200: 0.915289\n",
      "Feature 201: 1.041897\n",
      "Feature 202: 0.124298\n",
      "Feature 203: 6.946864\n",
      "Feature 204: 0.075905\n",
      "Feature 205: 0.166525\n",
      "Feature 206: 0.068885\n",
      "Feature 207: 0.197269\n",
      "Feature 208: 0.724353\n",
      "Feature 209: 0.215357\n",
      "Feature 210: 0.348401\n",
      "Feature 211: 0.072600\n",
      "Feature 212: 0.508874\n",
      "Feature 213: 1.250235\n",
      "Feature 214: 1.132343\n",
      "Feature 215: 1.500261\n",
      "Feature 216: 0.955265\n",
      "Feature 217: 0.044044\n",
      "Feature 218: 0.290236\n",
      "Feature 219: 0.144541\n",
      "Feature 220: 1.993474\n",
      "Feature 221: 2.305045\n",
      "Feature 222: 3.995462\n",
      "Feature 223: 3.568989\n",
      "Feature 224: 0.157611\n",
      "Feature 225: 0.069839\n",
      "Feature 226: 0.238613\n",
      "Feature 227: 0.703188\n",
      "Feature 228: 0.036081\n",
      "Feature 229: 0.995548\n",
      "Feature 230: 0.010265\n",
      "Feature 231: 0.313891\n",
      "Feature 232: 0.000000\n",
      "Feature 233: 10.532212\n",
      "Feature 234: 13.309850\n",
      "Feature 235: 0.390000\n",
      "Feature 236: 0.023951\n",
      "Feature 237: 16.471482\n",
      "Feature 238: 12.111671\n",
      "Feature 239: 11.238240\n",
      "Feature 240: 3.566599\n",
      "Feature 241: 0.954959\n",
      "Feature 242: 0.091706\n",
      "Feature 243: 0.089894\n",
      "Feature 244: 0.679251\n",
      "Feature 245: 0.017486\n",
      "Feature 246: 0.748582\n",
      "Feature 247: 0.004133\n",
      "Feature 248: 0.746957\n",
      "Feature 249: 2.445081\n",
      "Feature 250: 0.541195\n",
      "Feature 251: 0.015342\n",
      "Feature 252: 12.611603\n",
      "Feature 253: 2.471365\n",
      "Feature 254: 1.993072\n",
      "Feature 255: 4.412508\n",
      "Feature 256: 0.016854\n",
      "Feature 257: 0.149559\n",
      "Feature 258: 0.051325\n",
      "Feature 259: 2.465901\n",
      "Feature 260: 1.886313\n",
      "Feature 261: 1.888899\n",
      "Feature 262: 3.721306\n",
      "Feature 263: 2.606902\n",
      "Feature 264: 1.866491\n",
      "Feature 265: 2.456198\n",
      "Feature 266: 4.489323\n",
      "Feature 267: 0.072984\n",
      "Feature 268: 1.404879\n",
      "Feature 269: 1.875109\n",
      "Feature 270: 3.878951\n",
      "Feature 271: 0.071592\n",
      "Feature 272: 1.491176\n",
      "Feature 273: 0.831863\n",
      "Feature 274: 0.071592\n",
      "Feature 275: 26.948493\n",
      "Feature 276: 3.187286\n",
      "Feature 277: 0.015393\n",
      "Feature 278: 2.906825\n",
      "Feature 279: 0.744256\n",
      "Feature 280: 0.565421\n",
      "Feature 281: 0.074060\n",
      "Feature 282: 0.126238\n",
      "Feature 283: 0.153521\n",
      "Feature 284: 0.305954\n",
      "Feature 285: 1.714113\n",
      "Feature 286: 2.336203\n",
      "Feature 287: 0.038672\n",
      "Feature 288: 0.268180\n",
      "Feature 289: 1.354083\n",
      "Feature 290: 4.769640\n",
      "Feature 291: 0.254764\n",
      "Feature 292: 0.014721\n",
      "Feature 293: 0.122424\n",
      "Feature 294: 0.671870\n",
      "Feature 295: 1.915348\n",
      "Feature 296: 0.105552\n",
      "Feature 297: 6.912664\n",
      "Feature 298: 0.608989\n",
      "Feature 299: 0.168372\n",
      "Feature 300: 0.072470\n",
      "Feature 301: 0.022917\n",
      "Feature 302: 0.708219\n",
      "Feature 303: 0.017520\n",
      "Feature 304: 0.260984\n",
      "Feature 305: 0.818664\n",
      "Feature 306: 0.702523\n",
      "Feature 307: 1.168325\n",
      "Feature 308: 0.277840\n",
      "Feature 309: 1.354748\n",
      "Feature 310: 3.506677\n",
      "Feature 311: 0.109845\n",
      "Feature 312: 0.063688\n",
      "Feature 313: 0.586938\n",
      "Feature 314: 2.789994\n",
      "Feature 315: 2.849598\n",
      "Feature 316: 1.426422\n",
      "Feature 317: 0.317916\n",
      "Feature 318: 0.076182\n",
      "Feature 319: 0.042260\n",
      "Feature 320: 3.599260\n",
      "Feature 321: 0.818115\n",
      "Feature 322: 0.300097\n",
      "Feature 323: 0.660940\n",
      "Feature 324: 0.000881\n",
      "Feature 325: 1.085974\n",
      "Feature 326: 0.000119\n",
      "Feature 327: 19.016748\n",
      "Feature 328: 22.982985\n",
      "Feature 329: 0.030734\n",
      "Feature 330: 3.546965\n",
      "Feature 331: 19.740379\n",
      "Feature 332: 18.668558\n",
      "Feature 333: 17.929045\n",
      "Feature 334: 7.638346\n",
      "Feature 335: 0.376993\n",
      "Feature 336: 0.639661\n",
      "Feature 337: 0.086775\n",
      "Feature 338: 1.732436\n",
      "Feature 339: 0.280795\n",
      "Feature 340: 2.117792\n",
      "Feature 341: 0.070554\n",
      "Feature 342: 2.232838\n",
      "Feature 343: 2.707722\n",
      "Feature 344: 0.342456\n",
      "Feature 345: 0.019653\n",
      "Feature 346: 9.358515\n",
      "Feature 347: 2.721760\n",
      "Feature 348: 1.742150\n",
      "Feature 349: 4.971403\n",
      "Feature 350: 0.001164\n",
      "Feature 351: 0.109016\n",
      "Feature 352: 0.065240\n",
      "Feature 353: 5.765957\n",
      "Feature 354: 2.073466\n",
      "Feature 355: 4.822508\n",
      "Feature 356: 4.351006\n",
      "Feature 357: 2.291531\n",
      "Feature 358: 5.586966\n",
      "Feature 359: 0.002327\n",
      "Feature 360: 0.141265\n",
      "Feature 361: 0.342368\n",
      "Feature 362: 3.100241\n",
      "Feature 363: 1.276370\n",
      "Feature 364: 4.953975\n",
      "Feature 365: 0.071592\n",
      "Feature 366: 2.480439\n",
      "Feature 367: 0.209545\n",
      "Feature 368: 0.133208\n",
      "Feature 369: 0.893845\n",
      "Feature 370: 3.001492\n",
      "Feature 371: 1.491432\n",
      "Feature 372: 1.011174\n",
      "Feature 373: 0.209344\n",
      "Feature 374: 3.372588\n",
      "Feature 375: 0.585462\n",
      "Feature 376: 0.070480\n",
      "Feature 377: 0.306815\n",
      "Feature 378: 0.590454\n",
      "Feature 379: 1.939492\n",
      "Feature 380: 0.019524\n",
      "Feature 381: 0.065229\n",
      "Feature 382: 0.993493\n",
      "Feature 383: 1.414570\n",
      "Feature 384: 0.335238\n",
      "Feature 385: 27.575798\n",
      "Feature 386: 4.735442\n",
      "Feature 387: 0.092577\n",
      "Feature 388: 2.113299\n",
      "Feature 389: 0.292789\n",
      "Feature 390: 0.169336\n",
      "Feature 391: 0.053329\n",
      "Feature 392: 0.199265\n",
      "Feature 393: 0.726201\n",
      "Feature 394: 0.170105\n",
      "Feature 395: 0.446285\n",
      "Feature 396: 0.050180\n",
      "Feature 397: 0.964194\n",
      "Feature 398: 4.079056\n",
      "Feature 399: 4.132766\n",
      "Feature 400: 0.763523\n",
      "Feature 401: 3.309721\n",
      "Feature 402: 0.240956\n",
      "Feature 403: 3.014839\n",
      "Feature 404: 0.114547\n",
      "Feature 405: 0.024151\n",
      "Feature 406: 3.769782\n",
      "Feature 407: 5.333954\n",
      "Feature 408: 0.033240\n",
      "Feature 409: 3.690430\n",
      "Feature 410: 5.335509\n",
      "Feature 411: 0.780759\n",
      "Feature 412: 3.300226\n",
      "Feature 413: 5.357960\n",
      "Feature 414: 0.839387\n",
      "Feature 415: 0.909450\n",
      "Feature 416: 0.629574\n",
      "Feature 417: 1.073105\n",
      "Feature 418: 2.905983\n",
      "Feature 419: 0.360338\n",
      "Feature 420: 0.292280\n",
      "Feature 421: 2.547000\n",
      "Feature 422: 0.010422\n",
      "Feature 423: 2.553285\n",
      "Feature 424: 0.432293\n",
      "Feature 425: 3.293741\n",
      "Feature 426: 0.004293\n",
      "Feature 427: 0.586390\n",
      "Feature 428: 1.627713\n",
      "Feature 429: 4.222486\n",
      "Feature 430: 1.888142\n",
      "Feature 431: 4.363766\n",
      "Feature 432: 1.271100\n",
      "Feature 433: 3.864747\n",
      "Feature 434: 3.467675\n",
      "Feature 435: 0.055982\n",
      "Feature 436: 0.045952\n",
      "Feature 437: 0.039666\n",
      "Feature 438: 0.027037\n",
      "Feature 439: 1.962685\n",
      "Feature 440: 1.521691\n",
      "Feature 441: 0.011019\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finding F Score for the features\n",
    "for i in range (len(sel_fs.scores_)):\n",
    "    print('Feature %d: %f' % (i,sel_fs.scores_[i] ))\n",
    "#plot the scores\n",
    "plt.bar([i for i in range(len(sel_fs.scores_))], sel_fs.scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78876419",
   "metadata": {},
   "source": [
    "#Here we can see some features stand out perhaps because of them being more relevant than others, with higher test statistic value.\n",
    "higher the value better. So going forward with feature score greater than 1.0, dropping the rest lower scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e6582c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 8, 9, 19, 20, 21, 22, 23, 24, 25, 28, 39, 54, 66, 67, 68, 69, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 114, 115, 125, 126, 128, 129, 130, 131, 132, 137, 140, 141, 142, 143, 144, 145, 146, 147, 150, 151, 153, 158, 159, 161, 164, 168, 171, 174, 175, 177, 179, 182, 183, 184, 185, 186, 187, 188, 189, 190, 192, 193, 194, 196, 198, 199, 200, 201, 202, 203, 205, 207, 208, 210, 217, 218, 220, 221, 222, 223, 224, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 242, 243, 245, 246, 247, 248, 250, 251, 253, 256, 260, 263, 266, 267, 269, 271, 274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 288, 290, 291, 292, 293, 294, 295, 296, 297, 299, 301, 302, 303, 304, 311, 314, 316, 317, 327, 328, 329, 330, 331, 332, 333, 335, 339, 344, 345, 355, 360, 361, 363, 365, 368, 369, 370, 371, 372, 373, 374, 375, 379, 383, 384, 385, 386, 388, 390, 399, 400, 405, 408, 410, 415, 416, 421, 423, 429, 431, 435, 436, 438, 439, 440, 441]\n"
     ]
    }
   ],
   "source": [
    "#Dropping lower f scores \n",
    "drop_fs = [column for column in X_df.columns if any(X_df[column] < 0.1)]\n",
    "print(drop_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58134d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>430</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>437</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>1.500500</td>\n",
       "      <td>0.945500</td>\n",
       "      <td>202.439600</td>\n",
       "      <td>7.955800</td>\n",
       "      <td>414.871000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.452417</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>533.8500</td>\n",
       "      <td>2.1113</td>\n",
       "      <td>8.95</td>\n",
       "      <td>3.0624</td>\n",
       "      <td>1.6765</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>2.3630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>1.496600</td>\n",
       "      <td>0.962700</td>\n",
       "      <td>200.547000</td>\n",
       "      <td>10.154800</td>\n",
       "      <td>414.734700</td>\n",
       "      <td>...</td>\n",
       "      <td>2.452417</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>535.0164</td>\n",
       "      <td>2.4335</td>\n",
       "      <td>5.92</td>\n",
       "      <td>2.0111</td>\n",
       "      <td>1.1065</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>4.4447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>1.443600</td>\n",
       "      <td>0.961500</td>\n",
       "      <td>202.017900</td>\n",
       "      <td>9.515700</td>\n",
       "      <td>416.707500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411900</td>\n",
       "      <td>68.848900</td>\n",
       "      <td>535.0245</td>\n",
       "      <td>2.0293</td>\n",
       "      <td>11.21</td>\n",
       "      <td>4.0923</td>\n",
       "      <td>2.0952</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>3.1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>1.488200</td>\n",
       "      <td>0.962900</td>\n",
       "      <td>201.848200</td>\n",
       "      <td>9.605200</td>\n",
       "      <td>422.289400</td>\n",
       "      <td>...</td>\n",
       "      <td>2.729000</td>\n",
       "      <td>25.036300</td>\n",
       "      <td>530.5682</td>\n",
       "      <td>2.0253</td>\n",
       "      <td>9.33</td>\n",
       "      <td>2.8971</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>2.0544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>1.503100</td>\n",
       "      <td>0.956900</td>\n",
       "      <td>201.942400</td>\n",
       "      <td>10.566100</td>\n",
       "      <td>420.592500</td>\n",
       "      <td>...</td>\n",
       "      <td>2.452417</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>532.0155</td>\n",
       "      <td>2.0275</td>\n",
       "      <td>8.83</td>\n",
       "      <td>3.1776</td>\n",
       "      <td>1.6597</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>99.3032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>1.342400</td>\n",
       "      <td>0.957900</td>\n",
       "      <td>203.986700</td>\n",
       "      <td>11.769200</td>\n",
       "      <td>419.340400</td>\n",
       "      <td>...</td>\n",
       "      <td>1.884400</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>536.3418</td>\n",
       "      <td>2.0153</td>\n",
       "      <td>7.98</td>\n",
       "      <td>2.6401</td>\n",
       "      <td>1.4879</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>2.8669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>1.433300</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>204.017300</td>\n",
       "      <td>9.162000</td>\n",
       "      <td>405.817800</td>\n",
       "      <td>...</td>\n",
       "      <td>1.708900</td>\n",
       "      <td>20.911800</td>\n",
       "      <td>537.9264</td>\n",
       "      <td>2.1814</td>\n",
       "      <td>5.48</td>\n",
       "      <td>1.9077</td>\n",
       "      <td>1.0187</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>2.6238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>1.462862</td>\n",
       "      <td>0.964353</td>\n",
       "      <td>199.956809</td>\n",
       "      <td>9.005371</td>\n",
       "      <td>413.086035</td>\n",
       "      <td>...</td>\n",
       "      <td>4.319700</td>\n",
       "      <td>29.095400</td>\n",
       "      <td>530.3709</td>\n",
       "      <td>2.3435</td>\n",
       "      <td>6.49</td>\n",
       "      <td>2.1760</td>\n",
       "      <td>1.2237</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>3.0590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>1.462200</td>\n",
       "      <td>0.969400</td>\n",
       "      <td>197.244800</td>\n",
       "      <td>9.735400</td>\n",
       "      <td>401.915300</td>\n",
       "      <td>...</td>\n",
       "      <td>1.884400</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>534.3936</td>\n",
       "      <td>1.9098</td>\n",
       "      <td>9.13</td>\n",
       "      <td>3.2524</td>\n",
       "      <td>1.7085</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>3.5662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>1.462862</td>\n",
       "      <td>0.964353</td>\n",
       "      <td>199.956809</td>\n",
       "      <td>9.005371</td>\n",
       "      <td>413.086035</td>\n",
       "      <td>...</td>\n",
       "      <td>3.263900</td>\n",
       "      <td>21.112800</td>\n",
       "      <td>528.7918</td>\n",
       "      <td>2.0831</td>\n",
       "      <td>6.81</td>\n",
       "      <td>2.2727</td>\n",
       "      <td>1.2878</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>3.6275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 235 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1          2       4         5         7         10   \\\n",
       "0     3030.93  2564.00  2187.7333  1.3602   97.6133  1.500500  0.945500   \n",
       "1     3095.78  2465.14  2230.4222  0.8294  102.3433  1.496600  0.962700   \n",
       "2     2932.61  2559.94  2186.4111  1.5102   95.4878  1.443600  0.961500   \n",
       "3     2988.72  2479.90  2199.0333  1.3204  104.2367  1.488200  0.962900   \n",
       "4     3032.24  2502.87  2233.3667  1.5334  100.3967  1.503100  0.956900   \n",
       "...       ...      ...        ...     ...       ...       ...       ...   \n",
       "1562  2899.41  2464.36  2179.7333  1.4843   82.2467  1.342400  0.957900   \n",
       "1563  3052.31  2522.55  2198.5667  0.8763   98.4689  1.433300  0.961800   \n",
       "1564  2978.81  2379.78  2206.3000  0.8236   99.4122  1.462862  0.964353   \n",
       "1565  2894.92  2532.01  2177.0333  1.5726   98.7978  1.462200  0.969400   \n",
       "1566  2944.92  2450.76  2195.4444  1.5978   85.1011  1.462862  0.964353   \n",
       "\n",
       "             11         12          13   ...       424        425       426  \\\n",
       "0     202.439600   7.955800  414.871000  ...  2.452417  21.117674  533.8500   \n",
       "1     200.547000  10.154800  414.734700  ...  2.452417  21.117674  535.0164   \n",
       "2     202.017900   9.515700  416.707500  ...  0.411900  68.848900  535.0245   \n",
       "3     201.848200   9.605200  422.289400  ...  2.729000  25.036300  530.5682   \n",
       "4     201.942400  10.566100  420.592500  ...  2.452417  21.117674  532.0155   \n",
       "...          ...        ...         ...  ...       ...        ...       ...   \n",
       "1562  203.986700  11.769200  419.340400  ...  1.884400  15.466200  536.3418   \n",
       "1563  204.017300   9.162000  405.817800  ...  1.708900  20.911800  537.9264   \n",
       "1564  199.956809   9.005371  413.086035  ...  4.319700  29.095400  530.3709   \n",
       "1565  197.244800   9.735400  401.915300  ...  1.884400  15.466200  534.3936   \n",
       "1566  199.956809   9.005371  413.086035  ...  3.263900  21.112800  528.7918   \n",
       "\n",
       "         427    428     430     432      433     434      437  \n",
       "0     2.1113   8.95  3.0624  1.6765  14.9509  0.5005   2.3630  \n",
       "1     2.4335   5.92  2.0111  1.1065  10.9003  0.5019   4.4447  \n",
       "2     2.0293  11.21  4.0923  2.0952   9.2721  0.4958   3.1745  \n",
       "3     2.0253   9.33  2.8971  1.7585   8.5831  0.4990   2.0544  \n",
       "4     2.0275   8.83  3.1776  1.6597  10.9698  0.4800  99.3032  \n",
       "...      ...    ...     ...     ...      ...     ...      ...  \n",
       "1562  2.0153   7.98  2.6401  1.4879  11.7256  0.4988   2.8669  \n",
       "1563  2.1814   5.48  1.9077  1.0187  17.8379  0.4975   2.6238  \n",
       "1564  2.3435   6.49  2.1760  1.2237  17.7267  0.4987   3.0590  \n",
       "1565  1.9098   9.13  3.2524  1.7085  19.2104  0.5004   3.5662  \n",
       "1566  2.0831   6.81  2.2727  1.2878  22.9183  0.4987   3.6275  \n",
       "\n",
       "[1567 rows x 235 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1 = X_df.drop(drop_fs,axis = 1)\n",
    "X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2fd9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on dropping higher lower F scores we finally got 235 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34757e6b",
   "metadata": {},
   "source": [
    "Features with very low standard deviations are dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2076a4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 206)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether features have zero standard deviation and drop them,\n",
    "X_2=X_1\n",
    "for i in -np.sort(-X_2.std().index):\n",
    "    if (X_2.std()[i] < 0.1):\n",
    "        #print (i , \" : \", X_2.std()[i])\n",
    "        X_2 = X_2.drop([i],axis=1)\n",
    "X_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e61ee5",
   "metadata": {},
   "source": [
    "Features that show high correlations are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a42d501a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>422</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>430</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>437</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143840</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.007058</td>\n",
       "      <td>0.030675</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>0.017691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057316</td>\n",
       "      <td>0.060010</td>\n",
       "      <td>0.049862</td>\n",
       "      <td>0.018953</td>\n",
       "      <td>0.023166</td>\n",
       "      <td>0.013678</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.013228</td>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.023589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.143840</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.025564</td>\n",
       "      <td>0.034062</td>\n",
       "      <td>0.037667</td>\n",
       "      <td>0.087315</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.042938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.017051</td>\n",
       "      <td>0.025490</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.037932</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.002273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004756</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>0.136225</td>\n",
       "      <td>0.018326</td>\n",
       "      <td>0.006476</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.021878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050484</td>\n",
       "      <td>0.050434</td>\n",
       "      <td>0.064282</td>\n",
       "      <td>0.037070</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.028705</td>\n",
       "      <td>0.015752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011014</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074368</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.017523</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.027696</td>\n",
       "      <td>0.005273</td>\n",
       "      <td>0.081983</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.012264</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.001616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.025564</td>\n",
       "      <td>0.136225</td>\n",
       "      <td>0.074368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058982</td>\n",
       "      <td>0.055333</td>\n",
       "      <td>0.039815</td>\n",
       "      <td>0.040015</td>\n",
       "      <td>0.033196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015393</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.070722</td>\n",
       "      <td>0.017264</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>0.007783</td>\n",
       "      <td>0.007409</td>\n",
       "      <td>0.012342</td>\n",
       "      <td>0.039517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.013678</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>0.036757</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.024789</td>\n",
       "      <td>0.014068</td>\n",
       "      <td>0.055371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054146</td>\n",
       "      <td>0.042853</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>0.274167</td>\n",
       "      <td>0.151217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993689</td>\n",
       "      <td>0.994772</td>\n",
       "      <td>0.863768</td>\n",
       "      <td>0.017179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.012264</td>\n",
       "      <td>0.007783</td>\n",
       "      <td>0.032908</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.024032</td>\n",
       "      <td>0.014005</td>\n",
       "      <td>0.050808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052457</td>\n",
       "      <td>0.041095</td>\n",
       "      <td>0.014923</td>\n",
       "      <td>0.307529</td>\n",
       "      <td>0.138441</td>\n",
       "      <td>0.993689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991738</td>\n",
       "      <td>0.851784</td>\n",
       "      <td>0.016812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.013228</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>0.007409</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.023509</td>\n",
       "      <td>0.014167</td>\n",
       "      <td>0.054269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050525</td>\n",
       "      <td>0.039613</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.360498</td>\n",
       "      <td>0.136232</td>\n",
       "      <td>0.994772</td>\n",
       "      <td>0.991738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.859278</td>\n",
       "      <td>0.017147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>0.008601</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.028705</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.012342</td>\n",
       "      <td>0.031434</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.040682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063683</td>\n",
       "      <td>0.051925</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.247655</td>\n",
       "      <td>0.121115</td>\n",
       "      <td>0.863768</td>\n",
       "      <td>0.851784</td>\n",
       "      <td>0.859278</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.023589</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.039517</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011237</td>\n",
       "      <td>0.010758</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.010143</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.017179</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>0.023910</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         4         5         11        12   \\\n",
       "0    1.000000  0.143840  0.004756  0.011014  0.002270  0.010368  0.007058   \n",
       "1    0.143840  1.000000  0.005767  0.001636  0.025564  0.034062  0.037667   \n",
       "2    0.004756  0.005767  1.000000  0.095891  0.136225  0.018326  0.006476   \n",
       "4    0.011014  0.001636  0.095891  1.000000  0.074368  0.002707  0.017523   \n",
       "5    0.002270  0.025564  0.136225  0.074368  1.000000  0.058982  0.055333   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "428  0.013678  0.001753  0.000518  0.012024  0.009292  0.036757  0.000807   \n",
       "430  0.015206  0.001303  0.001342  0.012264  0.007783  0.032908  0.000409   \n",
       "432  0.013228  0.002570  0.002592  0.012163  0.007409  0.035743  0.000985   \n",
       "433  0.008601  0.010145  0.028705  0.004070  0.012342  0.031434  0.009505   \n",
       "437  0.023589  0.002273  0.015752  0.001616  0.039517  0.000523  0.002535   \n",
       "\n",
       "          13        14        16   ...       422       424       425  \\\n",
       "0    0.030675  0.005749  0.017691  ...  0.057316  0.060010  0.049862   \n",
       "1    0.087315  0.001878  0.042938  ...  0.009774  0.017051  0.025490   \n",
       "2    0.006115  0.000788  0.021878  ...  0.050484  0.050434  0.064282   \n",
       "4    0.011435  0.001763  0.001610  ...  0.015146  0.012944  0.027696   \n",
       "5    0.039815  0.040015  0.033196  ...  0.015393  0.016000  0.070722   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "428  0.024789  0.014068  0.055371  ...  0.054146  0.042853  0.013062   \n",
       "430  0.024032  0.014005  0.050808  ...  0.052457  0.041095  0.014923   \n",
       "432  0.023509  0.014167  0.054269  ...  0.050525  0.039613  0.012528   \n",
       "433  0.019152  0.004396  0.040682  ...  0.063683  0.051925  0.009732   \n",
       "437  0.017745  0.002643  0.001543  ...  0.011237  0.010758  0.055153   \n",
       "\n",
       "          426       427       428       430       432       433       437  \n",
       "0    0.018953  0.023166  0.013678  0.015206  0.013228  0.008601  0.023589  \n",
       "1    0.009000  0.037932  0.001753  0.001303  0.002570  0.010145  0.002273  \n",
       "2    0.037070  0.015600  0.000518  0.001342  0.002592  0.028705  0.015752  \n",
       "4    0.005273  0.081983  0.012024  0.012264  0.012163  0.004070  0.001616  \n",
       "5    0.017264  0.026100  0.009292  0.007783  0.007409  0.012342  0.039517  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "428  0.274167  0.151217  1.000000  0.993689  0.994772  0.863768  0.017179  \n",
       "430  0.307529  0.138441  0.993689  1.000000  0.991738  0.851784  0.016812  \n",
       "432  0.360498  0.136232  0.994772  0.991738  1.000000  0.859278  0.017147  \n",
       "433  0.247655  0.121115  0.863768  0.851784  0.859278  1.000000  0.023910  \n",
       "437  0.010143  0.006716  0.017179  0.016812  0.017147  0.023910  1.000000  \n",
       "\n",
       "[206 rows x 206 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping highly correlated\n",
    "cor = X_2.corr().abs()\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7865a923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0        1         2         4         5         11        12        13   \\\n",
      "0    NaN  0.14384  0.004756  0.011014  0.002270  0.010368  0.007058  0.030675   \n",
      "1    NaN      NaN  0.005767  0.001636  0.025564  0.034062  0.037667  0.087315   \n",
      "2    NaN      NaN       NaN  0.095891  0.136225  0.018326  0.006476  0.006115   \n",
      "4    NaN      NaN       NaN       NaN  0.074368  0.002707  0.017523  0.011435   \n",
      "5    NaN      NaN       NaN       NaN       NaN  0.058982  0.055333  0.039815   \n",
      "..   ...      ...       ...       ...       ...       ...       ...       ...   \n",
      "428  NaN      NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "430  NaN      NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "432  NaN      NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "433  NaN      NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "437  NaN      NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "          14        16   ...       422       424       425       426  \\\n",
      "0    0.005749  0.017691  ...  0.057316  0.060010  0.049862  0.018953   \n",
      "1    0.001878  0.042938  ...  0.009774  0.017051  0.025490  0.009000   \n",
      "2    0.000788  0.021878  ...  0.050484  0.050434  0.064282  0.037070   \n",
      "4    0.001763  0.001610  ...  0.015146  0.012944  0.027696  0.005273   \n",
      "5    0.040015  0.033196  ...  0.015393  0.016000  0.070722  0.017264   \n",
      "..        ...       ...  ...       ...       ...       ...       ...   \n",
      "428       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "430       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "432       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "433       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "437       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "\n",
      "          427       428       430       432       433       437  \n",
      "0    0.023166  0.013678  0.015206  0.013228  0.008601  0.023589  \n",
      "1    0.037932  0.001753  0.001303  0.002570  0.010145  0.002273  \n",
      "2    0.015600  0.000518  0.001342  0.002592  0.028705  0.015752  \n",
      "4    0.081983  0.012024  0.012264  0.012163  0.004070  0.001616  \n",
      "5    0.026100  0.009292  0.007783  0.007409  0.012342  0.039517  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "428       NaN       NaN  0.993689  0.994772  0.863768  0.017179  \n",
      "430       NaN       NaN       NaN  0.991738  0.851784  0.016812  \n",
      "432       NaN       NaN       NaN       NaN  0.859278  0.017147  \n",
      "433       NaN       NaN       NaN       NaN       NaN  0.023910  \n",
      "437       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "\n",
      "[206 rows x 206 columns]\n"
     ]
    }
   ],
   "source": [
    "up_tri = cor.where(np.triu(np.ones(cor.shape),k = 1).astype(np.bool)) \n",
    "print(up_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e9365c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[32, 33, 34, 37, 43, 46, 47, 55, 60, 61, 64, 110, 111, 113, 133, 136, 138, 139, 166, 169, 170, 172, 176, 178, 180, 206, 213, 214, 215, 216, 219, 225, 226, 227, 230, 240, 241, 252, 254, 255, 257, 258, 259, 261, 262, 264, 265, 268, 270, 272, 273, 283, 287, 289, 298, 300, 305, 306, 307, 308, 309, 310, 312, 313, 318, 319, 321, 322, 324, 325, 334, 338, 342, 343, 346, 348, 349, 350, 351, 352, 356, 357, 358, 362, 364, 366, 367, 376, 380, 382, 389, 391, 394, 396, 397, 407, 409, 412, 413, 422, 424, 430, 432, 433]\n"
     ]
    }
   ],
   "source": [
    "#dropping all features that show correlation greater than 0.7\n",
    "to_drop = [column for column in up_tri.columns if any(up_tri[column] > 0.7)]\n",
    "print()\n",
    "print (to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da276033",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>411</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>437</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>202.439600</td>\n",
       "      <td>7.955800</td>\n",
       "      <td>414.871000</td>\n",
       "      <td>10.043300</td>\n",
       "      <td>192.396300</td>\n",
       "      <td>...</td>\n",
       "      <td>39.8842</td>\n",
       "      <td>42.3877</td>\n",
       "      <td>262.729683</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>6.444985</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>533.8500</td>\n",
       "      <td>2.1113</td>\n",
       "      <td>8.95</td>\n",
       "      <td>2.3630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>200.547000</td>\n",
       "      <td>10.154800</td>\n",
       "      <td>414.734700</td>\n",
       "      <td>9.259900</td>\n",
       "      <td>191.287200</td>\n",
       "      <td>...</td>\n",
       "      <td>53.1836</td>\n",
       "      <td>18.1087</td>\n",
       "      <td>262.729683</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>6.444985</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>535.0164</td>\n",
       "      <td>2.4335</td>\n",
       "      <td>5.92</td>\n",
       "      <td>4.4447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>202.017900</td>\n",
       "      <td>9.515700</td>\n",
       "      <td>416.707500</td>\n",
       "      <td>9.314400</td>\n",
       "      <td>192.703500</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0713</td>\n",
       "      <td>24.7524</td>\n",
       "      <td>267.064000</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>68.848900</td>\n",
       "      <td>535.0245</td>\n",
       "      <td>2.0293</td>\n",
       "      <td>11.21</td>\n",
       "      <td>3.1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>201.848200</td>\n",
       "      <td>9.605200</td>\n",
       "      <td>422.289400</td>\n",
       "      <td>9.692400</td>\n",
       "      <td>192.155700</td>\n",
       "      <td>...</td>\n",
       "      <td>161.4081</td>\n",
       "      <td>62.7572</td>\n",
       "      <td>268.228000</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>7.320000</td>\n",
       "      <td>25.036300</td>\n",
       "      <td>530.5682</td>\n",
       "      <td>2.0253</td>\n",
       "      <td>9.33</td>\n",
       "      <td>2.0544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>201.942400</td>\n",
       "      <td>10.566100</td>\n",
       "      <td>420.592500</td>\n",
       "      <td>10.338700</td>\n",
       "      <td>191.603700</td>\n",
       "      <td>...</td>\n",
       "      <td>70.9706</td>\n",
       "      <td>22.0500</td>\n",
       "      <td>262.729683</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>6.444985</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>532.0155</td>\n",
       "      <td>2.0275</td>\n",
       "      <td>8.83</td>\n",
       "      <td>99.3032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>203.986700</td>\n",
       "      <td>11.769200</td>\n",
       "      <td>419.340400</td>\n",
       "      <td>10.239700</td>\n",
       "      <td>193.747000</td>\n",
       "      <td>...</td>\n",
       "      <td>85.1806</td>\n",
       "      <td>32.3812</td>\n",
       "      <td>264.272000</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>536.3418</td>\n",
       "      <td>2.0153</td>\n",
       "      <td>7.98</td>\n",
       "      <td>2.8669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>204.017300</td>\n",
       "      <td>9.162000</td>\n",
       "      <td>405.817800</td>\n",
       "      <td>10.228500</td>\n",
       "      <td>193.788900</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0176</td>\n",
       "      <td>32.1048</td>\n",
       "      <td>266.832000</td>\n",
       "      <td>0.625400</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>20.911800</td>\n",
       "      <td>537.9264</td>\n",
       "      <td>2.1814</td>\n",
       "      <td>5.48</td>\n",
       "      <td>2.6238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>199.956809</td>\n",
       "      <td>9.005371</td>\n",
       "      <td>413.086035</td>\n",
       "      <td>9.907603</td>\n",
       "      <td>190.047354</td>\n",
       "      <td>...</td>\n",
       "      <td>74.1541</td>\n",
       "      <td>13.0316</td>\n",
       "      <td>256.730000</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>11.090000</td>\n",
       "      <td>29.095400</td>\n",
       "      <td>530.3709</td>\n",
       "      <td>2.3435</td>\n",
       "      <td>6.49</td>\n",
       "      <td>3.0590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>197.244800</td>\n",
       "      <td>9.735400</td>\n",
       "      <td>401.915300</td>\n",
       "      <td>9.863000</td>\n",
       "      <td>187.381800</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0176</td>\n",
       "      <td>18.9966</td>\n",
       "      <td>264.272000</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>534.3936</td>\n",
       "      <td>1.9098</td>\n",
       "      <td>9.13</td>\n",
       "      <td>3.5662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>199.956809</td>\n",
       "      <td>9.005371</td>\n",
       "      <td>413.086035</td>\n",
       "      <td>9.907603</td>\n",
       "      <td>190.047354</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0176</td>\n",
       "      <td>21.4914</td>\n",
       "      <td>257.974000</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>8.420000</td>\n",
       "      <td>21.112800</td>\n",
       "      <td>528.7918</td>\n",
       "      <td>2.0831</td>\n",
       "      <td>6.81</td>\n",
       "      <td>3.6275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1          2       4         5           11         12   \\\n",
       "0     3030.93  2564.00  2187.7333  1.3602   97.6133  202.439600   7.955800   \n",
       "1     3095.78  2465.14  2230.4222  0.8294  102.3433  200.547000  10.154800   \n",
       "2     2932.61  2559.94  2186.4111  1.5102   95.4878  202.017900   9.515700   \n",
       "3     2988.72  2479.90  2199.0333  1.3204  104.2367  201.848200   9.605200   \n",
       "4     3032.24  2502.87  2233.3667  1.5334  100.3967  201.942400  10.566100   \n",
       "...       ...      ...        ...     ...       ...         ...        ...   \n",
       "1562  2899.41  2464.36  2179.7333  1.4843   82.2467  203.986700  11.769200   \n",
       "1563  3052.31  2522.55  2198.5667  0.8763   98.4689  204.017300   9.162000   \n",
       "1564  2978.81  2379.78  2206.3000  0.8236   99.4122  199.956809   9.005371   \n",
       "1565  2894.92  2532.01  2177.0333  1.5726   98.7978  197.244800   9.735400   \n",
       "1566  2944.92  2450.76  2195.4444  1.5978   85.1011  199.956809   9.005371   \n",
       "\n",
       "             13         14          16   ...       411      417         418  \\\n",
       "0     414.871000  10.043300  192.396300  ...   39.8842  42.3877  262.729683   \n",
       "1     414.734700   9.259900  191.287200  ...   53.1836  18.1087  262.729683   \n",
       "2     416.707500   9.314400  192.703500  ...   23.0713  24.7524  267.064000   \n",
       "3     422.289400   9.692400  192.155700  ...  161.4081  62.7572  268.228000   \n",
       "4     420.592500  10.338700  191.603700  ...   70.9706  22.0500  262.729683   \n",
       "...          ...        ...         ...  ...       ...      ...         ...   \n",
       "1562  419.340400  10.239700  193.747000  ...   85.1806  32.3812  264.272000   \n",
       "1563  405.817800  10.228500  193.788900  ...   27.0176  32.1048  266.832000   \n",
       "1564  413.086035   9.907603  190.047354  ...   74.1541  13.0316  256.730000   \n",
       "1565  401.915300   9.863000  187.381800  ...   27.0176  18.9966  264.272000   \n",
       "1566  413.086035   9.907603  190.047354  ...   27.0176  21.4914  257.974000   \n",
       "\n",
       "           419        420        425       426     427    428      437  \n",
       "0     0.679641   6.444985  21.117674  533.8500  2.1113   8.95   2.3630  \n",
       "1     0.679641   6.444985  21.117674  535.0164  2.4335   5.92   4.4447  \n",
       "2     0.903200   1.100000  68.848900  535.0245  2.0293  11.21   3.1745  \n",
       "3     0.651100   7.320000  25.036300  530.5682  2.0253   9.33   2.0544  \n",
       "4     0.679641   6.444985  21.117674  532.0155  2.0275   8.83  99.3032  \n",
       "...        ...        ...        ...       ...     ...    ...      ...  \n",
       "1562  0.567100   4.980000  15.466200  536.3418  2.0153   7.98   2.8669  \n",
       "1563  0.625400   4.560000  20.911800  537.9264  2.1814   5.48   2.6238  \n",
       "1564  0.820900  11.090000  29.095400  530.3709  2.3435   6.49   3.0590  \n",
       "1565  0.567100   4.980000  15.466200  534.3936  1.9098   9.13   3.5662  \n",
       "1566  0.619300   8.420000  21.112800  528.7918  2.0831   6.81   3.6275  \n",
       "\n",
       "[1567 rows x 102 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cor = X_2.drop(to_drop,axis = 1)\n",
    "X_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "777382cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on dropping highly correlated we got down the dataset to 124 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579ec22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9397709e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41e004dc",
   "metadata": {},
   "source": [
    "2D. Check for multi-collinearity in the data and take necessary action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b14847e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity can be detected  VIF (Variable Inflation Factors).\n",
    "#VIF starts at 1 and has no upper limit\n",
    "#VIF = 1, no correlation between the independent variable and the other variables\n",
    "#VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0eef1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#def calc_vif(x):\n",
    "\n",
    "     #Calculating VIF\n",
    "    #vif = pd.DataFrame()\n",
    "    #vif[\"variables\"] = x.columns\n",
    "    ##vif[\"VIF\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n",
    "\n",
    "    #return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf5d5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = X_cor.iloc[:,:-1]\n",
    "#calc_vif(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "084c4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping one of the correlated features will help in bringing down the multicollinearity between correlated features:\n",
    "#If you have two or more factors with a high VIF, remove one from the model because they supply redundant information\n",
    "#drop_colli= [column for column in x.columns if any(x[column] > 10)]\n",
    "#print(drop_colli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f56dc95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_colli = x.drop(drop_colli,axis = 1)\n",
    "#X_colli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af5ef41",
   "metadata": {},
   "source": [
    "2E. Make all relevant modifications on the data using both functional/logical reasoning/assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe126f",
   "metadata": {},
   "source": [
    "A. Write a for loop which will remove all the features with 20%+ Null values and impute rest with mean of the feature.\n",
    "B. Identify and drop the features which are having same value for all the rows. [3 Marks]\n",
    "C. Drop other features if required using relevant functional knowledge. Clearly justify the same. [2 Marks]\n",
    "D. Check for multi-collinearity in the data and take necessary action. [3 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60ecab",
   "metadata": {},
   "source": [
    "#The dataset was having a lot of missing values in almost every columns except the 'Time' and target classifier 'Pass/Fail'.\n",
    "As the dataset was very large it wasnt possible to get the exact view of it. So iterating through the entire dataset is required and all the Features with more than 20% null values are removed and the rest are imputed with mean. Since feature names are missing we cant guess the best method to imputebut to impute with the most common method of imputing with mean. It is important to handle Missing Data as many machine learning algorithms fail if there are missing data.\n",
    "\n",
    "#Features with same entries in their respective rows are dropped as it is completely not having any effect on target. \n",
    "A Machine Learning Model is nothing but a mathematical equation i.e.\n",
    "y = f(x)\n",
    "\n",
    "in which\n",
    "\n",
    "y = Target/Dependent Variable\n",
    "f(x) = Independent Variables(In our case a DataFrame containing the Train/Test Data)\n",
    "So technically, ML models quantifies and estimates about for what value of X, what will the probable output y.\n",
    "Assuming a single whole column is constant. So, a relationship between y and f(x=constant) is meaningless because for whatever value of y, that x will remain same.\n",
    "\n",
    "In this Analysis , feature selection based on anova F_test statistics was applied to determine the most important features contributing to pass/fail classification. this was done to reduce the dimensionality of the feature space before the classification process. Mainly ANOVA is used when input variables are  numeric and the target is a categorical / classification target variable. The results of this test can be used for feature selection where those faetures that are independant of the target variable can be removed from the dataset.The scikit - learn machine library provides f_classif() function  to select top k most relevant features through SelectKBest class. here we passed all the faetures that remained from duplicate dropping (444 columns) and found out the F scores  to get an idea of how many features we should select. larger the f_score value the better will its predictive nature on the target . so given the threshold of 1 we could drop \n",
    "the values less than 1. we could reduce the features to 152 .\n",
    "\n",
    "The remaining features were tested for high correlation where 0.5 and greater than that were dropped as severely correlated features will be increasing the risk of errors.thus reduced features to 69\n",
    "Then Using VIF (variance inflation Factor) multicollinearity was checked and features got dropped down to 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d733f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99507ec2",
   "metadata": {},
   "source": [
    "#Data analysis & visualisation: \n",
    "3A. Perform a detailed univariate Analysis with appropriate detailed comments after each analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "279c27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we combine the s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d4b418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combi = [X_cor, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aff32885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>437</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>202.439600</td>\n",
       "      <td>7.955800</td>\n",
       "      <td>414.871000</td>\n",
       "      <td>10.043300</td>\n",
       "      <td>192.396300</td>\n",
       "      <td>...</td>\n",
       "      <td>42.3877</td>\n",
       "      <td>262.729683</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>6.444985</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>533.8500</td>\n",
       "      <td>2.1113</td>\n",
       "      <td>8.95</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>200.547000</td>\n",
       "      <td>10.154800</td>\n",
       "      <td>414.734700</td>\n",
       "      <td>9.259900</td>\n",
       "      <td>191.287200</td>\n",
       "      <td>...</td>\n",
       "      <td>18.1087</td>\n",
       "      <td>262.729683</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>6.444985</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>535.0164</td>\n",
       "      <td>2.4335</td>\n",
       "      <td>5.92</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>202.017900</td>\n",
       "      <td>9.515700</td>\n",
       "      <td>416.707500</td>\n",
       "      <td>9.314400</td>\n",
       "      <td>192.703500</td>\n",
       "      <td>...</td>\n",
       "      <td>24.7524</td>\n",
       "      <td>267.064000</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>68.848900</td>\n",
       "      <td>535.0245</td>\n",
       "      <td>2.0293</td>\n",
       "      <td>11.21</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>201.848200</td>\n",
       "      <td>9.605200</td>\n",
       "      <td>422.289400</td>\n",
       "      <td>9.692400</td>\n",
       "      <td>192.155700</td>\n",
       "      <td>...</td>\n",
       "      <td>62.7572</td>\n",
       "      <td>268.228000</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>7.320000</td>\n",
       "      <td>25.036300</td>\n",
       "      <td>530.5682</td>\n",
       "      <td>2.0253</td>\n",
       "      <td>9.33</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>201.942400</td>\n",
       "      <td>10.566100</td>\n",
       "      <td>420.592500</td>\n",
       "      <td>10.338700</td>\n",
       "      <td>191.603700</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0500</td>\n",
       "      <td>262.729683</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>6.444985</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>532.0155</td>\n",
       "      <td>2.0275</td>\n",
       "      <td>8.83</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>203.986700</td>\n",
       "      <td>11.769200</td>\n",
       "      <td>419.340400</td>\n",
       "      <td>10.239700</td>\n",
       "      <td>193.747000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.3812</td>\n",
       "      <td>264.272000</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>536.3418</td>\n",
       "      <td>2.0153</td>\n",
       "      <td>7.98</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>204.017300</td>\n",
       "      <td>9.162000</td>\n",
       "      <td>405.817800</td>\n",
       "      <td>10.228500</td>\n",
       "      <td>193.788900</td>\n",
       "      <td>...</td>\n",
       "      <td>32.1048</td>\n",
       "      <td>266.832000</td>\n",
       "      <td>0.625400</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>20.911800</td>\n",
       "      <td>537.9264</td>\n",
       "      <td>2.1814</td>\n",
       "      <td>5.48</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>199.956809</td>\n",
       "      <td>9.005371</td>\n",
       "      <td>413.086035</td>\n",
       "      <td>9.907603</td>\n",
       "      <td>190.047354</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0316</td>\n",
       "      <td>256.730000</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>11.090000</td>\n",
       "      <td>29.095400</td>\n",
       "      <td>530.3709</td>\n",
       "      <td>2.3435</td>\n",
       "      <td>6.49</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>197.244800</td>\n",
       "      <td>9.735400</td>\n",
       "      <td>401.915300</td>\n",
       "      <td>9.863000</td>\n",
       "      <td>187.381800</td>\n",
       "      <td>...</td>\n",
       "      <td>18.9966</td>\n",
       "      <td>264.272000</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>534.3936</td>\n",
       "      <td>1.9098</td>\n",
       "      <td>9.13</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>199.956809</td>\n",
       "      <td>9.005371</td>\n",
       "      <td>413.086035</td>\n",
       "      <td>9.907603</td>\n",
       "      <td>190.047354</td>\n",
       "      <td>...</td>\n",
       "      <td>21.4914</td>\n",
       "      <td>257.974000</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>8.420000</td>\n",
       "      <td>21.112800</td>\n",
       "      <td>528.7918</td>\n",
       "      <td>2.0831</td>\n",
       "      <td>6.81</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0        1          2       4         5          11         12  \\\n",
       "0     3030.93  2564.00  2187.7333  1.3602   97.6133  202.439600   7.955800   \n",
       "1     3095.78  2465.14  2230.4222  0.8294  102.3433  200.547000  10.154800   \n",
       "2     2932.61  2559.94  2186.4111  1.5102   95.4878  202.017900   9.515700   \n",
       "3     2988.72  2479.90  2199.0333  1.3204  104.2367  201.848200   9.605200   \n",
       "4     3032.24  2502.87  2233.3667  1.5334  100.3967  201.942400  10.566100   \n",
       "...       ...      ...        ...     ...       ...         ...        ...   \n",
       "1562  2899.41  2464.36  2179.7333  1.4843   82.2467  203.986700  11.769200   \n",
       "1563  3052.31  2522.55  2198.5667  0.8763   98.4689  204.017300   9.162000   \n",
       "1564  2978.81  2379.78  2206.3000  0.8236   99.4122  199.956809   9.005371   \n",
       "1565  2894.92  2532.01  2177.0333  1.5726   98.7978  197.244800   9.735400   \n",
       "1566  2944.92  2450.76  2195.4444  1.5978   85.1011  199.956809   9.005371   \n",
       "\n",
       "              13         14          16  ...      417         418       419  \\\n",
       "0     414.871000  10.043300  192.396300  ...  42.3877  262.729683  0.679641   \n",
       "1     414.734700   9.259900  191.287200  ...  18.1087  262.729683  0.679641   \n",
       "2     416.707500   9.314400  192.703500  ...  24.7524  267.064000  0.903200   \n",
       "3     422.289400   9.692400  192.155700  ...  62.7572  268.228000  0.651100   \n",
       "4     420.592500  10.338700  191.603700  ...  22.0500  262.729683  0.679641   \n",
       "...          ...        ...         ...  ...      ...         ...       ...   \n",
       "1562  419.340400  10.239700  193.747000  ...  32.3812  264.272000  0.567100   \n",
       "1563  405.817800  10.228500  193.788900  ...  32.1048  266.832000  0.625400   \n",
       "1564  413.086035   9.907603  190.047354  ...  13.0316  256.730000  0.820900   \n",
       "1565  401.915300   9.863000  187.381800  ...  18.9966  264.272000  0.567100   \n",
       "1566  413.086035   9.907603  190.047354  ...  21.4914  257.974000  0.619300   \n",
       "\n",
       "            420        425       426     427    428      437  Pass/Fail  \n",
       "0      6.444985  21.117674  533.8500  2.1113   8.95   2.3630          1  \n",
       "1      6.444985  21.117674  535.0164  2.4335   5.92   4.4447          1  \n",
       "2      1.100000  68.848900  535.0245  2.0293  11.21   3.1745          0  \n",
       "3      7.320000  25.036300  530.5682  2.0253   9.33   2.0544          1  \n",
       "4      6.444985  21.117674  532.0155  2.0275   8.83  99.3032          1  \n",
       "...         ...        ...       ...     ...    ...      ...        ...  \n",
       "1562   4.980000  15.466200  536.3418  2.0153   7.98   2.8669          1  \n",
       "1563   4.560000  20.911800  537.9264  2.1814   5.48   2.6238          1  \n",
       "1564  11.090000  29.095400  530.3709  2.3435   6.49   3.0590          1  \n",
       "1565   4.980000  15.466200  534.3936  1.9098   9.13   3.5662          1  \n",
       "1566   8.420000  21.112800  528.7918  2.0831   6.81   3.6275          1  \n",
       "\n",
       "[1567 rows x 103 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df = pd.concat(combi,axis = 1)\n",
    "signal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bb9183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_df['Pass/Fail'] = signal_df['Pass/Fail'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db1cb56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             float64\n",
       "1             float64\n",
       "2             float64\n",
       "4             float64\n",
       "5             float64\n",
       "               ...   \n",
       "426           float64\n",
       "427           float64\n",
       "428           float64\n",
       "437           float64\n",
       "Pass/Fail    category\n",
       "Length: 103, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f81c0b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>411</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>437</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3014.452896</td>\n",
       "      <td>2495.850231</td>\n",
       "      <td>2200.547318</td>\n",
       "      <td>4.197013</td>\n",
       "      <td>101.112908</td>\n",
       "      <td>199.956809</td>\n",
       "      <td>9.005371</td>\n",
       "      <td>413.086035</td>\n",
       "      <td>9.907603</td>\n",
       "      <td>190.047354</td>\n",
       "      <td>...</td>\n",
       "      <td>57.746537</td>\n",
       "      <td>32.284956</td>\n",
       "      <td>262.729683</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>6.444985</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>530.523623</td>\n",
       "      <td>2.101836</td>\n",
       "      <td>28.450165</td>\n",
       "      <td>3.067826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>73.480613</td>\n",
       "      <td>80.227793</td>\n",
       "      <td>29.380932</td>\n",
       "      <td>56.103066</td>\n",
       "      <td>6.209271</td>\n",
       "      <td>3.255195</td>\n",
       "      <td>2.793916</td>\n",
       "      <td>17.204591</td>\n",
       "      <td>2.401563</td>\n",
       "      <td>2.778376</td>\n",
       "      <td>...</td>\n",
       "      <td>32.152263</td>\n",
       "      <td>19.020006</td>\n",
       "      <td>6.933639</td>\n",
       "      <td>0.110637</td>\n",
       "      <td>2.393043</td>\n",
       "      <td>9.280454</td>\n",
       "      <td>17.499736</td>\n",
       "      <td>0.275112</td>\n",
       "      <td>86.304681</td>\n",
       "      <td>3.576891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2743.240000</td>\n",
       "      <td>2158.750000</td>\n",
       "      <td>2060.660000</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>82.131100</td>\n",
       "      <td>182.094000</td>\n",
       "      <td>2.249300</td>\n",
       "      <td>333.448600</td>\n",
       "      <td>4.469600</td>\n",
       "      <td>169.177400</td>\n",
       "      <td>...</td>\n",
       "      <td>4.826900</td>\n",
       "      <td>7.236900</td>\n",
       "      <td>242.286000</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>3.250400</td>\n",
       "      <td>317.196400</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>1.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2966.665000</td>\n",
       "      <td>2452.885000</td>\n",
       "      <td>2181.099950</td>\n",
       "      <td>1.017700</td>\n",
       "      <td>97.937800</td>\n",
       "      <td>198.130950</td>\n",
       "      <td>7.096750</td>\n",
       "      <td>406.131000</td>\n",
       "      <td>9.568550</td>\n",
       "      <td>188.300650</td>\n",
       "      <td>...</td>\n",
       "      <td>34.147100</td>\n",
       "      <td>15.766900</td>\n",
       "      <td>262.101000</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>530.702700</td>\n",
       "      <td>1.982900</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>2.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3011.840000</td>\n",
       "      <td>2498.910000</td>\n",
       "      <td>2200.955600</td>\n",
       "      <td>1.317100</td>\n",
       "      <td>101.492200</td>\n",
       "      <td>199.537100</td>\n",
       "      <td>8.973900</td>\n",
       "      <td>412.262900</td>\n",
       "      <td>9.852000</td>\n",
       "      <td>189.675700</td>\n",
       "      <td>...</td>\n",
       "      <td>57.746537</td>\n",
       "      <td>29.780100</td>\n",
       "      <td>264.272000</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>6.290000</td>\n",
       "      <td>20.182400</td>\n",
       "      <td>532.398200</td>\n",
       "      <td>2.118600</td>\n",
       "      <td>8.650000</td>\n",
       "      <td>2.757700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3056.540000</td>\n",
       "      <td>2538.745000</td>\n",
       "      <td>2218.055500</td>\n",
       "      <td>1.529600</td>\n",
       "      <td>104.530000</td>\n",
       "      <td>202.006750</td>\n",
       "      <td>10.858700</td>\n",
       "      <td>419.082800</td>\n",
       "      <td>10.127750</td>\n",
       "      <td>192.178900</td>\n",
       "      <td>...</td>\n",
       "      <td>69.630650</td>\n",
       "      <td>44.113400</td>\n",
       "      <td>264.733000</td>\n",
       "      <td>0.738250</td>\n",
       "      <td>7.310000</td>\n",
       "      <td>23.035200</td>\n",
       "      <td>534.356400</td>\n",
       "      <td>2.290650</td>\n",
       "      <td>10.130000</td>\n",
       "      <td>3.294950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3356.350000</td>\n",
       "      <td>2846.440000</td>\n",
       "      <td>2315.266700</td>\n",
       "      <td>1114.536600</td>\n",
       "      <td>129.252200</td>\n",
       "      <td>272.045100</td>\n",
       "      <td>19.546500</td>\n",
       "      <td>824.927100</td>\n",
       "      <td>102.867700</td>\n",
       "      <td>215.597700</td>\n",
       "      <td>...</td>\n",
       "      <td>303.550000</td>\n",
       "      <td>101.114600</td>\n",
       "      <td>311.404000</td>\n",
       "      <td>1.298800</td>\n",
       "      <td>32.580000</td>\n",
       "      <td>84.802400</td>\n",
       "      <td>589.508200</td>\n",
       "      <td>2.739500</td>\n",
       "      <td>454.560000</td>\n",
       "      <td>99.303200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            4            5    \\\n",
       "count  1567.000000  1567.000000  1567.000000  1567.000000  1567.000000   \n",
       "mean   3014.452896  2495.850231  2200.547318     4.197013   101.112908   \n",
       "std      73.480613    80.227793    29.380932    56.103066     6.209271   \n",
       "min    2743.240000  2158.750000  2060.660000     0.681500    82.131100   \n",
       "25%    2966.665000  2452.885000  2181.099950     1.017700    97.937800   \n",
       "50%    3011.840000  2498.910000  2200.955600     1.317100   101.492200   \n",
       "75%    3056.540000  2538.745000  2218.055500     1.529600   104.530000   \n",
       "max    3356.350000  2846.440000  2315.266700  1114.536600   129.252200   \n",
       "\n",
       "               11           12           13           14           16   ...  \\\n",
       "count  1567.000000  1567.000000  1567.000000  1567.000000  1567.000000  ...   \n",
       "mean    199.956809     9.005371   413.086035     9.907603   190.047354  ...   \n",
       "std       3.255195     2.793916    17.204591     2.401563     2.778376  ...   \n",
       "min     182.094000     2.249300   333.448600     4.469600   169.177400  ...   \n",
       "25%     198.130950     7.096750   406.131000     9.568550   188.300650  ...   \n",
       "50%     199.537100     8.973900   412.262900     9.852000   189.675700  ...   \n",
       "75%     202.006750    10.858700   419.082800    10.127750   192.178900  ...   \n",
       "max     272.045100    19.546500   824.927100   102.867700   215.597700  ...   \n",
       "\n",
       "               411          417          418          419          420  \\\n",
       "count  1567.000000  1567.000000  1567.000000  1567.000000  1567.000000   \n",
       "mean     57.746537    32.284956   262.729683     0.679641     6.444985   \n",
       "std      32.152263    19.020006     6.933639     0.110637     2.393043   \n",
       "min       4.826900     7.236900   242.286000     0.304900     0.970000   \n",
       "25%      34.147100    15.766900   262.101000     0.567100     4.980000   \n",
       "50%      57.746537    29.780100   264.272000     0.679641     6.290000   \n",
       "75%      69.630650    44.113400   264.733000     0.738250     7.310000   \n",
       "max     303.550000   101.114600   311.404000     1.298800    32.580000   \n",
       "\n",
       "               425          426          427          428          437  \n",
       "count  1567.000000  1567.000000  1567.000000  1567.000000  1567.000000  \n",
       "mean     21.117674   530.523623     2.101836    28.450165     3.067826  \n",
       "std       9.280454    17.499736     0.275112    86.304681     3.576891  \n",
       "min       3.250400   317.196400     0.980200     3.540000     1.197500  \n",
       "25%      15.466200   530.702700     1.982900     7.500000     2.306500  \n",
       "50%      20.182400   532.398200     2.118600     8.650000     2.757700  \n",
       "75%      23.035200   534.356400     2.290650    10.130000     3.294950  \n",
       "max      84.802400   589.508200     2.739500   454.560000    99.303200  \n",
       "\n",
       "[8 rows x 102 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d0256d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Univariate Data Visualizations\n",
    "sns.distplot(signal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d62b339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x1e23e071a90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0718b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23de951c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23de952b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dea5ca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dea5e80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe8aca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe8aaf0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe7b310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe7b8b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fec9190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc90d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfbff10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc2640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfdd0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfb4c10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe868b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe86940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfd8100>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfd3520>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e077940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e077cd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e04a070>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e04a400>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23debe9a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23debed30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23de93310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23de936a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc6c40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc6fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfee5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfee940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc4ee0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fdd42b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe09850>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe09be0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fdd81c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fdd8550>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe20af0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe20e80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe2f460>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe2f7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe14d90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe38160>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0d1700>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0d1a90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0e1070>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0e1400>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0e09a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0e0d30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e098310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0986a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a0c40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a0fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a65b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a6940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e181ee0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1b72b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1b9850>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1b9be0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1821f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e182580>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e115b20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e115eb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e137490>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e137820>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e127dc0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e129190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240442730>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240442ac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24045a0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24045a460>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240465a00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240465d90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24047d370>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24047d700>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24183aca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241844070>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241850610>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418509a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24185ef40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241868310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418768b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241876c40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24188d220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24188d5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241899b50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241899ee0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418b14c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418b1850>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418bddf0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418c91c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418d3760>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418d3af0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418ec0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418ec460>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418f7a00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418f7d90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24190f370>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24190f700>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24191bca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241929070>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241932610>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419329a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24193ff40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24194b310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419598b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241959c40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24196d220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24196d5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24197ab50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24197aee0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419934c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241993850>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24199ddf0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419ab1c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419b7760>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419b7af0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419ce0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419ce460>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419daa00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419dad90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419f2370>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419f2700>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419feca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a09070>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a15640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a159d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a21f70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a2c340>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a3b8e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a3bc70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a51250>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a515e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a5cb80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a5cf10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a764f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a76880>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a81e50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a8c220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a977c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a97b50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ab1160>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ab14f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241abba90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241abbe20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ad3400>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ad3790>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aded30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aed100>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241af76a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241af7a30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b05fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b0f3a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b1b940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b1bcd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b332b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b33640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b3fbe0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b3ff70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b58550>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b588e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b63e80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b6f250>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b7b7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b7bb80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b94160>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b944f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b9fa90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b9fe20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bb7400>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bb7790>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bc1d30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bce100>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bda6a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bdaa30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241be6fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bf13a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bfe940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bfecd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c152b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c15640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c22be0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c22f70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c3c550>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c3c8e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c46e80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c51250>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c5d7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c5db80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c76190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c76520>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c81ac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c81e50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c97430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c977c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ca3d60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cb2130>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cbc6d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cbca60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cd3040>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cd33d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cdf970>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cdfd00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cf72e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cf7670>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x1e23e0712b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23df82790>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23de959a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23de95e20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfcfd30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfcfcd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe8a910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe82310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe7bdc0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe7bc10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc9a30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc91f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc24c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc2040>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfb49a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfb4a60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe9ad60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe9a3a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfd3490>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfd32e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0650a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e065430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e04a790>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e04ab20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23deba100>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23deba490>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23de93a30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23de93dc0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dff43a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dff4730>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfeecd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc40a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fdd4640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fdd49d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe09f70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fddf340>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fdd88e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fdd8c70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe48250>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe485e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe2fb80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe2ff10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe384f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe38880>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0d1e20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0eb1f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0e1790>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0e1b20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e081100>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e081490>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e098a30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e098dc0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a73a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a7730>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a6cd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1810a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1b7640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1b79d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1b9f70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1a9340>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e182910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e182ca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e13c280>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e13c610>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e137bb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e137f40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e129520>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1298b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240442e50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24044d220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24045a7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24045ab80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240471160>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2404714f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24047da90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24047de20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241844400>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241844790>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241850d30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24185e100>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418686a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241868a30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241876fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418803a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24188d940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24188dcd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418a22b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418a2640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418b1be0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418b1f70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418c9550>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418c98e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418d3e80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418df250>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418ec7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418ecb80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241903160>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419034f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24190fa90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24190fe20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241929400>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241929790>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241932d30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24193f100>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24194b6a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24194ba30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241959fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419623a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24196d940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24196dcd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419862b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241986640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241993be0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241993f70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419ab550>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419ab8e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419b7e80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419c1250>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419ce7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419ceb80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419e5160>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419e54f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419f2a90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419f2e20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a09400>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a09790>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a15d60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a21130>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a2c6d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a2ca60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a45040>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a453d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a51970>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a51d00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a672e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a67670>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a76c10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a76fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a8c5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a8c940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a97ee0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aa42b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ab1880>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ab1c10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ac81f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ac8580>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ad3b20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ad3eb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aed490>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aed820>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241af7dc0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b05190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b0f730>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b0fac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b270a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b27430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b339d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b33d60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b4a340>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b4a6d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b58c70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b63040>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b6f5e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b6f970>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b7bf10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b842e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b94880>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b94c10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ba91f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ba9580>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bb7b20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bb7eb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bce490>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bce820>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bdadc0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241be6190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bf1730>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bf1ac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c090a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c09430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c159d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c15d60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c2c340>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c2c6d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c3cc70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c46040>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c515e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c51970>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c5df10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c68310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c768b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c76c40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c8d220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c8d5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c97b50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c97ee0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cb24c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cb2850>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cbcdf0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cc71c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cd3760>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cd3af0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cec0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cec460>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cf7a00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cf7d90>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x1e23cfc05b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23df82670>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dea58b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe8a430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe7b550>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfbaaf0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfbffa0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfdd7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe860d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfd8970>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0775b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e065ca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23debe610>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23debaf40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc68b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfee220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc4b50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe094c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fddfdf0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe20760>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe2f0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe14a00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0d1370>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0ebca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0e0610>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e081f40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a08b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a6220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e181b50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1b94c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1a9df0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e115790>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e137100>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e127a30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2404423a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24044dd00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240465670>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240471fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24183a910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241850280>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24185ebb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241876520>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241880e50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418997c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418b1130>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418bda60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418d33d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418dfd00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418f7670>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241903fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24191b910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241932280>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24193fbb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241959520>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241962e50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24197a7c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241993130>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24199da60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419b73d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419c1d00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419da670>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419e5fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419fe910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a152b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a21be0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a3b550>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a45e80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a5c7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a76160>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a81ac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a97430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aa4d90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241abb700>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ad3070>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ade9a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241af7310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b05c40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b1b5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b27ee0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b3f850>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b581c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b63af0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b7b460>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b84d90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b9f700>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bb7070>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bc19a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bda310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241be6c40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bfe5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c09ee0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c22850>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c3c1c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c46af0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c5d460>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c68dc0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c81730>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c970a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ca39d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cbc340>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cc7c70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cdf5e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cecf10>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x1e23df82190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dea5100>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfcf8b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe82b50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfbab50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc9be0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc20d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfb47f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe9a910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2402e5e20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0657c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e04aeb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23deba820>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc6190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dff4ac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc4430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fdd4d60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fddf6d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe20040>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe48970>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe142e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe38c10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0eb580>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0e1eb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e081820>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a0190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a7ac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e181430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1b7d60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1a96d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e115070>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e13c9a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e127310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e129c40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24044d5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24045af10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240471880>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24183a1f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241844b20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24185e490>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241868dc0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241880730>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418990a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418a29d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418bd340>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418c9c70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418df5e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418ecf10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241903880>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24191b1f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241929b20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24193f490>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24194bdc0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241962730>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24197a0a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419869d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24199d340>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419abc70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419c15e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419cef10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419e5880>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419fe1f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a09b20>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a214c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a2cdf0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a45760>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a5c0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a67a00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a813a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a8ccd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aa4640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ab1fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ac8910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ade280>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aedbb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b05520>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b0fe50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b277c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b3f130>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b4aa60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b633d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b6fd00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b84670>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b94fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ba9910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bc1280>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bcebb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241be6520>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bf1e50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c097c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c22130>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c2ca60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c463d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c51d00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c686a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c76fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c8d940>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ca32b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cb2be0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cc7550>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cd3e80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cec7f0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241d04160>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x1e23df82430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dea5310>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe8a0a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe82370>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfbafa0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfbf580>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfddac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe86c40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfd8580>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e077220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e065910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23debe280>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23debabb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc6520>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dff4e50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23dfc47c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe09130>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fddfa60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe203d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe48d00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe14670>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23fe38fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0eb910>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0e0280>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e081bb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a0520>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e0a7e50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1817c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1b9130>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1a9a60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e115400>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e13cd30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e1276a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e23e129fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24044d970>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2404652e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e240471c10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24183a580>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241844eb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24185e820>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241876190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241880ac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241899430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418a2d60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418bd6d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418d3040>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418df970>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2418f72e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241903c10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24191b580>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241929eb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24193f820>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241959190>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241962ac0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24197a430>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241986d60>,\n",
       "  <matplotlib.lines.Line2D at 0x1e24199d6d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419b7040>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419c1970>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419da2e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419e5c10>,\n",
       "  <matplotlib.lines.Line2D at 0x1e2419fe580>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a09eb0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a21850>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a3b1c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a45af0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a5c460>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a67d90>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a81730>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241a970a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aa49d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241abb370>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ac8ca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ade610>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241aedf40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b058b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b1b220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b27b50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b3f4c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b4adf0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b63760>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b7b0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b84a00>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241b9f370>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ba9ca0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bc1610>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bcef40>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241be68b0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241bfe220>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c09b50>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c224c0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c2cdf0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c46760>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c5d0d0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c68a30>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c813a0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241c8dcd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241ca3640>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cb2f70>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cc78e0>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cdf250>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241cecb80>,\n",
       "  <matplotlib.lines.Line2D at 0x1e241d044f0>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize =(10, 10))\n",
    " \n",
    "# Creating plot\n",
    "plt.boxplot(signal_df)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "519d3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above boxplots features  are having outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b8de5",
   "metadata": {},
   "source": [
    "3B. Perform bivariate and multivariate analysis with appropriate detailed comments after each analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dcd60a",
   "metadata": {},
   "source": [
    "#bivariate and multivate analysis\n",
    "The term bivariate analysis refers to the analysis of two variables.\n",
    "\n",
    "The purpose of bivariate analysis is to understand the relationship between two variables\n",
    "\n",
    "There are three common ways to perform bivariate analysis:\n",
    "\n",
    "1. Scatterplots\n",
    "\n",
    "2. Correlation Coefficients\n",
    "\n",
    "3. Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "786a039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e242147610>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Scatterplot\n",
    "plt.scatter(signal_df[29], signal_df[75])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a1739f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The scatter plot indicates moderatively strong positive correlation between variables feature number 7 and feature no:118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b1cab94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>411</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>437</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.143840</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>-0.011014</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.010368</td>\n",
       "      <td>-0.007058</td>\n",
       "      <td>0.030675</td>\n",
       "      <td>-0.005749</td>\n",
       "      <td>0.017691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030640</td>\n",
       "      <td>0.037917</td>\n",
       "      <td>-0.009583</td>\n",
       "      <td>-0.007999</td>\n",
       "      <td>0.058916</td>\n",
       "      <td>0.049862</td>\n",
       "      <td>-0.018953</td>\n",
       "      <td>-0.023166</td>\n",
       "      <td>0.013678</td>\n",
       "      <td>0.023589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.143840</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>-0.001636</td>\n",
       "      <td>-0.025564</td>\n",
       "      <td>0.034062</td>\n",
       "      <td>-0.037667</td>\n",
       "      <td>-0.087315</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>0.042938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024335</td>\n",
       "      <td>-0.025492</td>\n",
       "      <td>0.037138</td>\n",
       "      <td>0.019643</td>\n",
       "      <td>-0.013734</td>\n",
       "      <td>-0.025490</td>\n",
       "      <td>-0.009000</td>\n",
       "      <td>-0.037932</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.002273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004756</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>-0.136225</td>\n",
       "      <td>0.018326</td>\n",
       "      <td>0.006476</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>-0.000788</td>\n",
       "      <td>0.021878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004217</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.013310</td>\n",
       "      <td>-0.020346</td>\n",
       "      <td>0.050365</td>\n",
       "      <td>0.064282</td>\n",
       "      <td>-0.037070</td>\n",
       "      <td>-0.015600</td>\n",
       "      <td>-0.000518</td>\n",
       "      <td>0.015752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.011014</td>\n",
       "      <td>-0.001636</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.074368</td>\n",
       "      <td>-0.002707</td>\n",
       "      <td>-0.017523</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>-0.001763</td>\n",
       "      <td>-0.001610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022289</td>\n",
       "      <td>-0.025806</td>\n",
       "      <td>0.041957</td>\n",
       "      <td>0.032932</td>\n",
       "      <td>-0.010889</td>\n",
       "      <td>0.027696</td>\n",
       "      <td>0.005273</td>\n",
       "      <td>-0.081983</td>\n",
       "      <td>-0.012024</td>\n",
       "      <td>-0.001616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002270</td>\n",
       "      <td>-0.025564</td>\n",
       "      <td>-0.136225</td>\n",
       "      <td>-0.074368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058982</td>\n",
       "      <td>0.055333</td>\n",
       "      <td>0.039815</td>\n",
       "      <td>0.040015</td>\n",
       "      <td>0.033196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066665</td>\n",
       "      <td>-0.041082</td>\n",
       "      <td>-0.046295</td>\n",
       "      <td>-0.022655</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>-0.070722</td>\n",
       "      <td>0.017264</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>-0.039517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.049862</td>\n",
       "      <td>-0.025490</td>\n",
       "      <td>0.064282</td>\n",
       "      <td>0.027696</td>\n",
       "      <td>-0.070722</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>-0.009876</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011317</td>\n",
       "      <td>-0.015424</td>\n",
       "      <td>-0.002704</td>\n",
       "      <td>0.168009</td>\n",
       "      <td>0.318655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015133</td>\n",
       "      <td>-0.016163</td>\n",
       "      <td>-0.013062</td>\n",
       "      <td>0.055153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>-0.018953</td>\n",
       "      <td>-0.009000</td>\n",
       "      <td>-0.037070</td>\n",
       "      <td>0.005273</td>\n",
       "      <td>0.017264</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>-0.007288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016982</td>\n",
       "      <td>0.022871</td>\n",
       "      <td>-0.003118</td>\n",
       "      <td>-0.017709</td>\n",
       "      <td>0.012683</td>\n",
       "      <td>-0.015133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.095755</td>\n",
       "      <td>-0.274167</td>\n",
       "      <td>0.010143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>-0.023166</td>\n",
       "      <td>-0.037932</td>\n",
       "      <td>-0.015600</td>\n",
       "      <td>-0.081983</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>-0.035122</td>\n",
       "      <td>0.022778</td>\n",
       "      <td>-0.019711</td>\n",
       "      <td>-0.006873</td>\n",
       "      <td>-0.035912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055318</td>\n",
       "      <td>0.046282</td>\n",
       "      <td>-0.114101</td>\n",
       "      <td>0.033850</td>\n",
       "      <td>-0.053841</td>\n",
       "      <td>-0.016163</td>\n",
       "      <td>-0.095755</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.151217</td>\n",
       "      <td>0.006716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.013678</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>-0.000518</td>\n",
       "      <td>-0.012024</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>0.036757</td>\n",
       "      <td>-0.000807</td>\n",
       "      <td>-0.024789</td>\n",
       "      <td>-0.014068</td>\n",
       "      <td>0.055371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048871</td>\n",
       "      <td>-0.045176</td>\n",
       "      <td>0.031277</td>\n",
       "      <td>0.070029</td>\n",
       "      <td>0.046813</td>\n",
       "      <td>-0.013062</td>\n",
       "      <td>-0.274167</td>\n",
       "      <td>-0.151217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.023589</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>-0.001616</td>\n",
       "      <td>-0.039517</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>-0.001543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026898</td>\n",
       "      <td>0.023803</td>\n",
       "      <td>-0.010457</td>\n",
       "      <td>-0.013711</td>\n",
       "      <td>-0.011065</td>\n",
       "      <td>0.055153</td>\n",
       "      <td>0.010143</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>-0.017179</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         4         5         11        12   \\\n",
       "0    1.000000 -0.143840  0.004756 -0.011014  0.002270  0.010368 -0.007058   \n",
       "1   -0.143840  1.000000  0.005767 -0.001636 -0.025564  0.034062 -0.037667   \n",
       "2    0.004756  0.005767  1.000000  0.095891 -0.136225  0.018326  0.006476   \n",
       "4   -0.011014 -0.001636  0.095891  1.000000 -0.074368 -0.002707 -0.017523   \n",
       "5    0.002270 -0.025564 -0.136225 -0.074368  1.000000  0.058982  0.055333   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "425  0.049862 -0.025490  0.064282  0.027696 -0.070722 -0.007246  0.002705   \n",
       "426 -0.018953 -0.009000 -0.037070  0.005273  0.017264 -0.000639  0.020110   \n",
       "427 -0.023166 -0.037932 -0.015600 -0.081983  0.026100 -0.035122  0.022778   \n",
       "428  0.013678  0.001753 -0.000518 -0.012024  0.009292  0.036757 -0.000807   \n",
       "437  0.023589  0.002273  0.015752 -0.001616 -0.039517  0.000523  0.002535   \n",
       "\n",
       "          13        14        16   ...       411       417       418  \\\n",
       "0    0.030675 -0.005749  0.017691  ...  0.030640  0.037917 -0.009583   \n",
       "1   -0.087315 -0.001878  0.042938  ... -0.024335 -0.025492  0.037138   \n",
       "2    0.006115 -0.000788  0.021878  ... -0.004217  0.025862  0.013310   \n",
       "4    0.011435 -0.001763 -0.001610  ... -0.022289 -0.025806  0.041957   \n",
       "5    0.039815  0.040015  0.033196  ...  0.066665 -0.041082 -0.046295   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "425  0.000864 -0.009876  0.000047  ... -0.011317 -0.015424 -0.002704   \n",
       "426  0.011928  0.007679 -0.007288  ...  0.016982  0.022871 -0.003118   \n",
       "427 -0.019711 -0.006873 -0.035912  ...  0.055318  0.046282 -0.114101   \n",
       "428 -0.024789 -0.014068  0.055371  ... -0.048871 -0.045176  0.031277   \n",
       "437  0.017745  0.002643 -0.001543  ...  0.026898  0.023803 -0.010457   \n",
       "\n",
       "          419       420       425       426       427       428       437  \n",
       "0   -0.007999  0.058916  0.049862 -0.018953 -0.023166  0.013678  0.023589  \n",
       "1    0.019643 -0.013734 -0.025490 -0.009000 -0.037932  0.001753  0.002273  \n",
       "2   -0.020346  0.050365  0.064282 -0.037070 -0.015600 -0.000518  0.015752  \n",
       "4    0.032932 -0.010889  0.027696  0.005273 -0.081983 -0.012024 -0.001616  \n",
       "5   -0.022655  0.012260 -0.070722  0.017264  0.026100  0.009292 -0.039517  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "425  0.168009  0.318655  1.000000 -0.015133 -0.016163 -0.013062  0.055153  \n",
       "426 -0.017709  0.012683 -0.015133  1.000000 -0.095755 -0.274167  0.010143  \n",
       "427  0.033850 -0.053841 -0.016163 -0.095755  1.000000 -0.151217  0.006716  \n",
       "428  0.070029  0.046813 -0.013062 -0.274167 -0.151217  1.000000 -0.017179  \n",
       "437 -0.013711 -0.011065  0.055153  0.010143  0.006716 -0.017179  1.000000  \n",
       "\n",
       "[102 rows x 102 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_corr=signal_df.corr()\n",
    "signal_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1ec1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we dont find very strong positive or negative correlation among these features, as we have already dropped highly correlated features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "909e987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e5f0e",
   "metadata": {},
   "source": [
    "4. Data pre-processing:\n",
    "A. Segregate predictors vs target attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0924e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= signal_df.drop(['Pass/Fail'],axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47898504",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = signal_df['Pass/Fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76240cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1463\n",
       "0     104\n",
       "Name: Pass/Fail, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "781a69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE for data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1c39ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy = 'minority')\n",
    "X_sm, y_sm = smote.fit_sample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "452c4908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1463\n",
       "1    1463\n",
       "Name: Pass/Fail, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56162146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2926, 102), (2926,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sm.shape,y_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7234d020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2048, 102), (878, 102))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X_sm,y_sm,test_size=0.3, random_state=0,stratify = y_sm)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28a67c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>411</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>437</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3008.904082</td>\n",
       "      <td>2494.574529</td>\n",
       "      <td>2199.929165</td>\n",
       "      <td>3.502142</td>\n",
       "      <td>101.310537</td>\n",
       "      <td>200.003793</td>\n",
       "      <td>8.656725</td>\n",
       "      <td>412.938362</td>\n",
       "      <td>9.955095</td>\n",
       "      <td>190.048691</td>\n",
       "      <td>...</td>\n",
       "      <td>59.271139</td>\n",
       "      <td>33.240363</td>\n",
       "      <td>262.125413</td>\n",
       "      <td>0.678039</td>\n",
       "      <td>6.515783</td>\n",
       "      <td>21.710099</td>\n",
       "      <td>531.273584</td>\n",
       "      <td>2.098273</td>\n",
       "      <td>23.123164</td>\n",
       "      <td>3.143911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>74.848564</td>\n",
       "      <td>70.671486</td>\n",
       "      <td>27.926147</td>\n",
       "      <td>49.086636</td>\n",
       "      <td>5.311953</td>\n",
       "      <td>2.908899</td>\n",
       "      <td>2.493343</td>\n",
       "      <td>12.057633</td>\n",
       "      <td>2.113287</td>\n",
       "      <td>2.484315</td>\n",
       "      <td>...</td>\n",
       "      <td>29.769451</td>\n",
       "      <td>17.649426</td>\n",
       "      <td>6.647369</td>\n",
       "      <td>0.091924</td>\n",
       "      <td>2.159647</td>\n",
       "      <td>8.969151</td>\n",
       "      <td>14.094841</td>\n",
       "      <td>0.277156</td>\n",
       "      <td>72.128337</td>\n",
       "      <td>3.183495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2743.240000</td>\n",
       "      <td>2162.870000</td>\n",
       "      <td>2060.660000</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>82.131100</td>\n",
       "      <td>182.094000</td>\n",
       "      <td>2.376074</td>\n",
       "      <td>389.467100</td>\n",
       "      <td>8.036700</td>\n",
       "      <td>169.177400</td>\n",
       "      <td>...</td>\n",
       "      <td>5.385900</td>\n",
       "      <td>7.236900</td>\n",
       "      <td>242.580127</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.250400</td>\n",
       "      <td>317.196400</td>\n",
       "      <td>0.984700</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>1.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2956.086782</td>\n",
       "      <td>2457.425353</td>\n",
       "      <td>2180.888900</td>\n",
       "      <td>1.076346</td>\n",
       "      <td>98.733140</td>\n",
       "      <td>198.439216</td>\n",
       "      <td>6.882514</td>\n",
       "      <td>407.226931</td>\n",
       "      <td>9.616182</td>\n",
       "      <td>188.556612</td>\n",
       "      <td>...</td>\n",
       "      <td>41.820816</td>\n",
       "      <td>18.167653</td>\n",
       "      <td>260.759245</td>\n",
       "      <td>0.607050</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>531.060071</td>\n",
       "      <td>1.990374</td>\n",
       "      <td>7.655372</td>\n",
       "      <td>2.373475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2999.611731</td>\n",
       "      <td>2497.201267</td>\n",
       "      <td>2199.051272</td>\n",
       "      <td>1.313955</td>\n",
       "      <td>101.587800</td>\n",
       "      <td>199.752391</td>\n",
       "      <td>8.643161</td>\n",
       "      <td>412.466331</td>\n",
       "      <td>9.897450</td>\n",
       "      <td>189.796988</td>\n",
       "      <td>...</td>\n",
       "      <td>57.746537</td>\n",
       "      <td>31.367839</td>\n",
       "      <td>263.427861</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>6.391062</td>\n",
       "      <td>20.998033</td>\n",
       "      <td>532.614138</td>\n",
       "      <td>2.138354</td>\n",
       "      <td>8.708596</td>\n",
       "      <td>2.824567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3050.592500</td>\n",
       "      <td>2531.540707</td>\n",
       "      <td>2216.500000</td>\n",
       "      <td>1.486900</td>\n",
       "      <td>104.106846</td>\n",
       "      <td>201.691830</td>\n",
       "      <td>10.321587</td>\n",
       "      <td>418.116596</td>\n",
       "      <td>10.187959</td>\n",
       "      <td>191.770554</td>\n",
       "      <td>...</td>\n",
       "      <td>70.084177</td>\n",
       "      <td>44.113400</td>\n",
       "      <td>264.548054</td>\n",
       "      <td>0.721727</td>\n",
       "      <td>7.481799</td>\n",
       "      <td>24.352025</td>\n",
       "      <td>534.326400</td>\n",
       "      <td>2.286223</td>\n",
       "      <td>10.036818</td>\n",
       "      <td>3.352750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3356.350000</td>\n",
       "      <td>2839.460000</td>\n",
       "      <td>2315.266700</td>\n",
       "      <td>1114.536600</td>\n",
       "      <td>129.252200</td>\n",
       "      <td>272.045100</td>\n",
       "      <td>19.546500</td>\n",
       "      <td>824.927100</td>\n",
       "      <td>102.867700</td>\n",
       "      <td>215.597700</td>\n",
       "      <td>...</td>\n",
       "      <td>303.550000</td>\n",
       "      <td>101.114600</td>\n",
       "      <td>311.404000</td>\n",
       "      <td>1.298800</td>\n",
       "      <td>19.610000</td>\n",
       "      <td>73.899000</td>\n",
       "      <td>589.508200</td>\n",
       "      <td>2.739500</td>\n",
       "      <td>454.560000</td>\n",
       "      <td>99.303200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            4            5    \\\n",
       "count  2048.000000  2048.000000  2048.000000  2048.000000  2048.000000   \n",
       "mean   3008.904082  2494.574529  2199.929165     3.502142   101.310537   \n",
       "std      74.848564    70.671486    27.926147    49.086636     5.311953   \n",
       "min    2743.240000  2162.870000  2060.660000     0.681500    82.131100   \n",
       "25%    2956.086782  2457.425353  2180.888900     1.076346    98.733140   \n",
       "50%    2999.611731  2497.201267  2199.051272     1.313955   101.587800   \n",
       "75%    3050.592500  2531.540707  2216.500000     1.486900   104.106846   \n",
       "max    3356.350000  2839.460000  2315.266700  1114.536600   129.252200   \n",
       "\n",
       "               11           12           13           14           16   ...  \\\n",
       "count  2048.000000  2048.000000  2048.000000  2048.000000  2048.000000  ...   \n",
       "mean    200.003793     8.656725   412.938362     9.955095   190.048691  ...   \n",
       "std       2.908899     2.493343    12.057633     2.113287     2.484315  ...   \n",
       "min     182.094000     2.376074   389.467100     8.036700   169.177400  ...   \n",
       "25%     198.439216     6.882514   407.226931     9.616182   188.556612  ...   \n",
       "50%     199.752391     8.643161   412.466331     9.897450   189.796988  ...   \n",
       "75%     201.691830    10.321587   418.116596    10.187959   191.770554  ...   \n",
       "max     272.045100    19.546500   824.927100   102.867700   215.597700  ...   \n",
       "\n",
       "               411          417          418          419          420  \\\n",
       "count  2048.000000  2048.000000  2048.000000  2048.000000  2048.000000   \n",
       "mean     59.271139    33.240363   262.125413     0.678039     6.515783   \n",
       "std      29.769451    17.649426     6.647369     0.091924     2.159647   \n",
       "min       5.385900     7.236900   242.580127     0.304900     1.120000   \n",
       "25%      41.820816    18.167653   260.759245     0.607050     4.980000   \n",
       "50%      57.746537    31.367839   263.427861     0.679641     6.391062   \n",
       "75%      70.084177    44.113400   264.548054     0.721727     7.481799   \n",
       "max     303.550000   101.114600   311.404000     1.298800    19.610000   \n",
       "\n",
       "               425          426          427          428          437  \n",
       "count  2048.000000  2048.000000  2048.000000  2048.000000  2048.000000  \n",
       "mean     21.710099   531.273584     2.098273    23.123164     3.143911  \n",
       "std       8.969151    14.094841     0.277156    72.128337     3.183495  \n",
       "min       3.250400   317.196400     0.984700     3.540000     1.284500  \n",
       "25%      15.466200   531.060071     1.990374     7.655372     2.373475  \n",
       "50%      20.998033   532.614138     2.138354     8.708596     2.824567  \n",
       "75%      24.352025   534.326400     2.286223    10.036818     3.352750  \n",
       "max      73.899000   589.508200     2.739500   454.560000    99.303200  \n",
       "\n",
       "[8 rows x 102 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the 5 point summary of train data\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b78197c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>411</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>437</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3004.284687</td>\n",
       "      <td>2496.436110</td>\n",
       "      <td>2200.008465</td>\n",
       "      <td>1.307812</td>\n",
       "      <td>101.456267</td>\n",
       "      <td>199.906700</td>\n",
       "      <td>8.597515</td>\n",
       "      <td>413.069662</td>\n",
       "      <td>9.879035</td>\n",
       "      <td>190.024353</td>\n",
       "      <td>...</td>\n",
       "      <td>59.697960</td>\n",
       "      <td>33.853412</td>\n",
       "      <td>262.153399</td>\n",
       "      <td>0.675300</td>\n",
       "      <td>6.471878</td>\n",
       "      <td>22.073397</td>\n",
       "      <td>530.622080</td>\n",
       "      <td>2.111834</td>\n",
       "      <td>18.409217</td>\n",
       "      <td>2.967300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>68.273486</td>\n",
       "      <td>67.788759</td>\n",
       "      <td>26.999354</td>\n",
       "      <td>0.391708</td>\n",
       "      <td>5.180891</td>\n",
       "      <td>2.219668</td>\n",
       "      <td>2.589442</td>\n",
       "      <td>15.848125</td>\n",
       "      <td>0.520403</td>\n",
       "      <td>2.261803</td>\n",
       "      <td>...</td>\n",
       "      <td>29.704598</td>\n",
       "      <td>18.242775</td>\n",
       "      <td>6.288971</td>\n",
       "      <td>0.087190</td>\n",
       "      <td>2.342647</td>\n",
       "      <td>9.760593</td>\n",
       "      <td>16.707152</td>\n",
       "      <td>0.270904</td>\n",
       "      <td>54.992164</td>\n",
       "      <td>1.027867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2787.490000</td>\n",
       "      <td>2158.750000</td>\n",
       "      <td>2060.660000</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>82.131100</td>\n",
       "      <td>191.077700</td>\n",
       "      <td>2.249300</td>\n",
       "      <td>333.448600</td>\n",
       "      <td>4.469600</td>\n",
       "      <td>181.111900</td>\n",
       "      <td>...</td>\n",
       "      <td>4.826900</td>\n",
       "      <td>7.236900</td>\n",
       "      <td>242.286000</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>3.342400</td>\n",
       "      <td>335.598200</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>4.940000</td>\n",
       "      <td>1.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2957.326491</td>\n",
       "      <td>2460.424498</td>\n",
       "      <td>2180.296221</td>\n",
       "      <td>1.045723</td>\n",
       "      <td>99.113300</td>\n",
       "      <td>198.445385</td>\n",
       "      <td>6.775402</td>\n",
       "      <td>407.486800</td>\n",
       "      <td>9.612576</td>\n",
       "      <td>188.596075</td>\n",
       "      <td>...</td>\n",
       "      <td>41.790800</td>\n",
       "      <td>17.566200</td>\n",
       "      <td>261.150000</td>\n",
       "      <td>0.613883</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>531.175400</td>\n",
       "      <td>1.995704</td>\n",
       "      <td>7.724437</td>\n",
       "      <td>2.331846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2999.410855</td>\n",
       "      <td>2500.855000</td>\n",
       "      <td>2198.420711</td>\n",
       "      <td>1.301500</td>\n",
       "      <td>101.460000</td>\n",
       "      <td>199.747894</td>\n",
       "      <td>8.475606</td>\n",
       "      <td>412.719923</td>\n",
       "      <td>9.892120</td>\n",
       "      <td>189.798133</td>\n",
       "      <td>...</td>\n",
       "      <td>57.746537</td>\n",
       "      <td>31.865508</td>\n",
       "      <td>263.397073</td>\n",
       "      <td>0.677073</td>\n",
       "      <td>6.204584</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>532.527148</td>\n",
       "      <td>2.144950</td>\n",
       "      <td>8.744915</td>\n",
       "      <td>2.752705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3043.765221</td>\n",
       "      <td>2534.466709</td>\n",
       "      <td>2216.636125</td>\n",
       "      <td>1.459691</td>\n",
       "      <td>103.948390</td>\n",
       "      <td>201.431206</td>\n",
       "      <td>10.355725</td>\n",
       "      <td>417.539932</td>\n",
       "      <td>10.148087</td>\n",
       "      <td>191.567769</td>\n",
       "      <td>...</td>\n",
       "      <td>70.878400</td>\n",
       "      <td>45.996005</td>\n",
       "      <td>264.363977</td>\n",
       "      <td>0.709127</td>\n",
       "      <td>7.316563</td>\n",
       "      <td>24.947003</td>\n",
       "      <td>534.172354</td>\n",
       "      <td>2.295461</td>\n",
       "      <td>10.130000</td>\n",
       "      <td>3.233890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3266.550000</td>\n",
       "      <td>2846.440000</td>\n",
       "      <td>2315.266700</td>\n",
       "      <td>4.197013</td>\n",
       "      <td>129.252200</td>\n",
       "      <td>208.633000</td>\n",
       "      <td>18.862600</td>\n",
       "      <td>817.003000</td>\n",
       "      <td>12.173859</td>\n",
       "      <td>199.123600</td>\n",
       "      <td>...</td>\n",
       "      <td>290.238954</td>\n",
       "      <td>99.037500</td>\n",
       "      <td>288.864000</td>\n",
       "      <td>1.197800</td>\n",
       "      <td>32.580000</td>\n",
       "      <td>84.802400</td>\n",
       "      <td>550.585500</td>\n",
       "      <td>2.739500</td>\n",
       "      <td>438.870000</td>\n",
       "      <td>9.295300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2           4           5    \\\n",
       "count   878.000000   878.000000   878.000000  878.000000  878.000000   \n",
       "mean   3004.284687  2496.436110  2200.008465    1.307812  101.456267   \n",
       "std      68.273486    67.788759    26.999354    0.391708    5.180891   \n",
       "min    2787.490000  2158.750000  2060.660000    0.681500   82.131100   \n",
       "25%    2957.326491  2460.424498  2180.296221    1.045723   99.113300   \n",
       "50%    2999.410855  2500.855000  2198.420711    1.301500  101.460000   \n",
       "75%    3043.765221  2534.466709  2216.636125    1.459691  103.948390   \n",
       "max    3266.550000  2846.440000  2315.266700    4.197013  129.252200   \n",
       "\n",
       "              11          12          13          14          16   ...  \\\n",
       "count  878.000000  878.000000  878.000000  878.000000  878.000000  ...   \n",
       "mean   199.906700    8.597515  413.069662    9.879035  190.024353  ...   \n",
       "std      2.219668    2.589442   15.848125    0.520403    2.261803  ...   \n",
       "min    191.077700    2.249300  333.448600    4.469600  181.111900  ...   \n",
       "25%    198.445385    6.775402  407.486800    9.612576  188.596075  ...   \n",
       "50%    199.747894    8.475606  412.719923    9.892120  189.798133  ...   \n",
       "75%    201.431206   10.355725  417.539932   10.148087  191.567769  ...   \n",
       "max    208.633000   18.862600  817.003000   12.173859  199.123600  ...   \n",
       "\n",
       "              411         417         418         419         420         425  \\\n",
       "count  878.000000  878.000000  878.000000  878.000000  878.000000  878.000000   \n",
       "mean    59.697960   33.853412  262.153399    0.675300    6.471878   22.073397   \n",
       "std     29.704598   18.242775    6.288971    0.087190    2.342647    9.760593   \n",
       "min      4.826900    7.236900  242.286000    0.500700    0.970000    3.342400   \n",
       "25%     41.790800   17.566200  261.150000    0.613883    4.980000   15.466200   \n",
       "50%     57.746537   31.865508  263.397073    0.677073    6.204584   21.117674   \n",
       "75%     70.878400   45.996005  264.363977    0.709127    7.316563   24.947003   \n",
       "max    290.238954   99.037500  288.864000    1.197800   32.580000   84.802400   \n",
       "\n",
       "              426         427         428         437  \n",
       "count  878.000000  878.000000  878.000000  878.000000  \n",
       "mean   530.622080    2.111834   18.409217    2.967300  \n",
       "std     16.707152    0.270904   54.992164    1.027867  \n",
       "min    335.598200    0.980200    4.940000    1.197500  \n",
       "25%    531.175400    1.995704    7.724437    2.331846  \n",
       "50%    532.527148    2.144950    8.744915    2.752705  \n",
       "75%    534.172354    2.295461   10.130000    3.233890  \n",
       "max    550.585500    2.739500  438.870000    9.295300  \n",
       "\n",
       "[8 rows x 102 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5 point summary of test data\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd3050",
   "metadata": {},
   "source": [
    "#the statistics have not changed when comparing with the test and train with the parent dataset but with the original dataset they show slight differences \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63673bdb",
   "metadata": {},
   "source": [
    "5. Model training, testing and tuning: \n",
    "A. Use any Supervised Learning technique to train a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466ae7d",
   "metadata": {},
   "source": [
    "5A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ecec77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)\n",
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bfd19339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0  Random Forest       0.986333             1.0      0.993   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score\n",
    "rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "pred = rf.predict(X_test_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "rf_score = rf.score(X_test_scaled, y_test)\n",
    "rf_recall = round(tp/(tp+fn), 3)   # Of all the signal , how many were truly recognised to have 'Pass'\n",
    "  \n",
    "rf_precision = round(tp/(tp+fp), 3)  # Of all the signal predicted , how many truly predicted as 'Pass'\n",
    "    \n",
    "rf_specificity =round(tn/(tn+fp),3) # Of all the signals, how many were recognised as truly 'Fail'\n",
    "\n",
    "\n",
    "train_pred = rf.predict(x_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "result = pd.DataFrame({'Model' : ['rf'], 'Test Accuracy' : [rf_score],'Train Accuracy':[train_acc], 'Precision' : [rf_precision],\n",
    "                      'True positive rate' : [rf_recall], 'True negative rate' : [rf_specificity],\n",
    "                       'False positive rate' :  [1-rf_specificity], })\n",
    "result.loc[0] = ['Random Forest', rf_score,train_acc, rf_precision, rf_recall, rf_specificity, 1 - rf_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10dc8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Pass       0.98      0.99      0.99       439\n",
      "        Fail       0.99      0.98      0.99       439\n",
      "\n",
      "    accuracy                           0.99       878\n",
      "   macro avg       0.99      0.99      0.99       878\n",
      "weighted avg       0.99      0.99      0.99       878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Pass', 'Fail']\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e8e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24ff3864",
   "metadata": {},
   "source": [
    "\n",
    "5.B. Use cross validation techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c849949c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9804878 , 0.9902439 , 0.9902439 , 0.9902439 , 0.98536585,\n",
       "       0.9804878 , 0.9804878 , 0.9804878 , 0.98039216, 0.97058824])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#k-fold cross validation( without stratification)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "num_folds = 10\n",
    "\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "results = cross_val_score(rf,x_train, y_train, cv=kfold)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07d4418b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829029172644667"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e0ef265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005907813697657298"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2d271f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation with stratification\n",
    "k = 10\n",
    "from sklearn.model_selection  import StratifiedKFold, cross_val_score\n",
    "stratified_kfold = StratifiedKFold(n_splits = k, shuffle =True, random_state = 55)\n",
    "results = cross_val_score(rf, x_train, y_train, cv = stratified_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "af2d3e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score\n",
      "Avearge:  0.9809516977522715\n",
      "Standard deviation:  0.007726346978102621\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score')\n",
    "print('Avearge: ', results.mean())\n",
    "print('Standard deviation: ', results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46caa9",
   "metadata": {},
   "source": [
    "5.C. Apply hyper-parameter tuning techniques to get the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db350b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = {'bootstrap': [True, False],\n",
    "        'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "         'min_samples_leaf': [1, 2, 4],\n",
    "         'min_samples_split': [2, 5, 10],\n",
    "         'n_estimators': [200, 400, 600]}\n",
    "\n",
    "gs = GridSearchCV(estimator = RandomForestClassifier(random_state=1), param_grid = grid, cv = 10, n_jobs = -1, verbose = 2 )\n",
    "\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "gs.best_params_\n",
    "\n",
    "\n",
    "#the code takes very long to run so not able to show the execution results. the Best parameters obtained are made use in the next code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9ed65a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0         Random Forest       0.986333             1.0      0.993   \n",
       "1  Random forest(tuned)       0.989749             1.0      0.993   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  \n",
       "1               0.986               0.993                0.007  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "forest_tuned = RandomForestClassifier( bootstrap = False,\n",
    "  max_depth = 30,\n",
    "  max_features =  'auto' ,\n",
    "  min_samples_leaf = 1,\n",
    "  min_samples_split = 2,\n",
    "  n_estimators = 200, random_state= 1)\n",
    "\n",
    "forest_tuned.fit(x_train, y_train)\n",
    "\n",
    "pred = forest_tuned.predict(X_test_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "forest_score = forest_tuned.score(X_test_scaled, y_test)\n",
    "forest_recall = round(tp/(tp+fn), 3)   # Of all the signal , how many were truly recognised to have 'Pass'\n",
    "forest_precision = round(tp/(tp+fp), 3) # Of all the signal predicted , how many truly predicted as 'Pass' \n",
    "forest_specificity =round(tn/(tn+fp),3) # Of all the signals, how many were recognised as truly 'Fail'\n",
    "\n",
    "train_pred = forest_tuned.predict(x_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "result.loc[1] = ['Random forest(tuned)', forest_score,train_acc, forest_precision, forest_recall, forest_specificity, 1 - forest_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b26a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5fbfd710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Pass       0.99      0.99      0.99       439\n",
      "        Fail       0.99      0.99      0.99       439\n",
      "\n",
      "    accuracy                           0.99       878\n",
      "   macro avg       0.99      0.99      0.99       878\n",
      "weighted avg       0.99      0.99      0.99       878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Pass', 'Fail']\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b50ea021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: \t1 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t102\n",
      "Rejected: \t0\n",
      "Iteration: \t2 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t102\n",
      "Rejected: \t0\n",
      "Iteration: \t3 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t102\n",
      "Rejected: \t0\n",
      "Iteration: \t4 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t102\n",
      "Rejected: \t0\n",
      "Iteration: \t5 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t102\n",
      "Rejected: \t0\n",
      "Iteration: \t6 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t102\n",
      "Rejected: \t0\n",
      "Iteration: \t7 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t102\n",
      "Rejected: \t0\n",
      "Iteration: \t8 / 100\n",
      "Confirmed: \t102\n",
      "Tentative: \t0\n",
      "Rejected: \t0\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t9 / 100\n",
      "Confirmed: \t102\n",
      "Tentative: \t0\n",
      "Rejected: \t0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BorutaPy(estimator=RandomForestClassifier(bootstrap=False, max_depth=30,\n",
       "                                          n_estimators=47,\n",
       "                                          random_state=RandomState(MT19937) at 0x1E243803340),\n",
       "         n_estimators='auto',\n",
       "         random_state=RandomState(MT19937) at 0x1E243803340, verbose=2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from boruta import BorutaPy\n",
    "boruta_selector = BorutaPy(forest_tuned,n_estimators = 'auto',verbose =2 ,random_state = 1)\n",
    "boruta_selector.fit(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5f7efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all 102 features were selected significant by BorutaPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce53deb",
   "metadata": {},
   "source": [
    "D. Use any other technique/method which can enhance the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ccaf46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "56f2e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "XScaled = pd.DataFrame(X_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c0c5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in XScaled.columns[:-1]:\n",
    "    Q1 = XScaled[col].quantile(0.25)\n",
    "    Q3 = XScaled[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_value = Q1 - (1.5 * IQR)\n",
    "    upper_value = Q3 + (1.5 * IQR)\n",
    "    \n",
    "    XScaled.loc[(XScaled[col]< lower_value) | ( XScaled[col] > upper_value), col] = XScaled[col].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "74108272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102]),\n",
       " [Text(1, 0, '0'),\n",
       "  Text(2, 0, '1'),\n",
       "  Text(3, 0, '2'),\n",
       "  Text(4, 0, '4'),\n",
       "  Text(5, 0, '5'),\n",
       "  Text(6, 0, '11'),\n",
       "  Text(7, 0, '12'),\n",
       "  Text(8, 0, '13'),\n",
       "  Text(9, 0, '14'),\n",
       "  Text(10, 0, '16'),\n",
       "  Text(11, 0, '17'),\n",
       "  Text(12, 0, '26'),\n",
       "  Text(13, 0, '27'),\n",
       "  Text(14, 0, '29'),\n",
       "  Text(15, 0, '30'),\n",
       "  Text(16, 0, '31'),\n",
       "  Text(17, 0, '35'),\n",
       "  Text(18, 0, '36'),\n",
       "  Text(19, 0, '38'),\n",
       "  Text(20, 0, '40'),\n",
       "  Text(21, 0, '41'),\n",
       "  Text(22, 0, '42'),\n",
       "  Text(23, 0, '44'),\n",
       "  Text(24, 0, '45'),\n",
       "  Text(25, 0, '50'),\n",
       "  Text(26, 0, '56'),\n",
       "  Text(27, 0, '57'),\n",
       "  Text(28, 0, '58'),\n",
       "  Text(29, 0, '59'),\n",
       "  Text(30, 0, '62'),\n",
       "  Text(31, 0, '63'),\n",
       "  Text(32, 0, '65'),\n",
       "  Text(33, 0, '75'),\n",
       "  Text(34, 0, '79'),\n",
       "  Text(35, 0, '81'),\n",
       "  Text(36, 0, '101'),\n",
       "  Text(37, 0, '103'),\n",
       "  Text(38, 0, '106'),\n",
       "  Text(39, 0, '108'),\n",
       "  Text(40, 0, '109'),\n",
       "  Text(41, 0, '112'),\n",
       "  Text(42, 0, '119'),\n",
       "  Text(43, 0, '120'),\n",
       "  Text(44, 0, '121'),\n",
       "  Text(45, 0, '122'),\n",
       "  Text(46, 0, '123'),\n",
       "  Text(47, 0, '124'),\n",
       "  Text(48, 0, '127'),\n",
       "  Text(49, 0, '134'),\n",
       "  Text(50, 0, '135'),\n",
       "  Text(51, 0, '148'),\n",
       "  Text(52, 0, '149'),\n",
       "  Text(53, 0, '152'),\n",
       "  Text(54, 0, '157'),\n",
       "  Text(55, 0, '160'),\n",
       "  Text(56, 0, '162'),\n",
       "  Text(57, 0, '163'),\n",
       "  Text(58, 0, '165'),\n",
       "  Text(59, 0, '167'),\n",
       "  Text(60, 0, '173'),\n",
       "  Text(61, 0, '181'),\n",
       "  Text(62, 0, '191'),\n",
       "  Text(63, 0, '195'),\n",
       "  Text(64, 0, '197'),\n",
       "  Text(65, 0, '204'),\n",
       "  Text(66, 0, '209'),\n",
       "  Text(67, 0, '211'),\n",
       "  Text(68, 0, '212'),\n",
       "  Text(69, 0, '315'),\n",
       "  Text(70, 0, '320'),\n",
       "  Text(71, 0, '323'),\n",
       "  Text(72, 0, '326'),\n",
       "  Text(73, 0, '336'),\n",
       "  Text(74, 0, '337'),\n",
       "  Text(75, 0, '340'),\n",
       "  Text(76, 0, '341'),\n",
       "  Text(77, 0, '347'),\n",
       "  Text(78, 0, '353'),\n",
       "  Text(79, 0, '354'),\n",
       "  Text(80, 0, '359'),\n",
       "  Text(81, 0, '377'),\n",
       "  Text(82, 0, '378'),\n",
       "  Text(83, 0, '381'),\n",
       "  Text(84, 0, '387'),\n",
       "  Text(85, 0, '392'),\n",
       "  Text(86, 0, '393'),\n",
       "  Text(87, 0, '395'),\n",
       "  Text(88, 0, '401'),\n",
       "  Text(89, 0, '402'),\n",
       "  Text(90, 0, '403'),\n",
       "  Text(91, 0, '404'),\n",
       "  Text(92, 0, '406'),\n",
       "  Text(93, 0, '411'),\n",
       "  Text(94, 0, '417'),\n",
       "  Text(95, 0, '418'),\n",
       "  Text(96, 0, '419'),\n",
       "  Text(97, 0, '420'),\n",
       "  Text(98, 0, '425'),\n",
       "  Text(99, 0, '426'),\n",
       "  Text(100, 0, '427'),\n",
       "  Text(101, 0, '428'),\n",
       "  Text(102, 0, '437')])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize= (15,15))\n",
    "XScaled.boxplot()\n",
    "plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d949bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.97258589e+03 -7.32434509e+02  4.00459970e+01 ... -1.02035565e+00\n",
      "  -7.31390428e+00  5.13609055e-01]\n",
      " [-7.32434509e+02  2.83290845e+03  2.61113310e+01 ... -4.53630470e-01\n",
      "   5.60987268e+00  7.39748048e-01]\n",
      " [ 4.00459970e+01  2.61113310e+01  5.67980327e+02 ... -2.37291277e-02\n",
      "   2.20921527e+00  7.31101053e-01]\n",
      " ...\n",
      " [-1.02035565e+00 -4.53630470e-01 -2.37291277e-02 ...  3.87202803e-02\n",
      "   2.96193634e-02  1.30126129e-02]\n",
      " [-7.31390428e+00  5.60987268e+00  2.20921527e+00 ...  2.96193634e-02\n",
      "   2.53642581e+00  1.15293150e-01]\n",
      " [ 5.13609055e-01  7.39748048e-01  7.31101053e-01 ...  1.30126129e-02\n",
      "   1.15293150e-01  7.41584121e+00]]\n"
     ]
    }
   ],
   "source": [
    "covMatrix = np.cov(XScaled,rowvar=False)\n",
    "print(covMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "178b8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen Vectors \n",
      "%s [[-2.53389718e-03 -1.30888965e-02 -7.26789867e-01 ...  1.49093312e-04\n",
      "  -4.92000110e-06  1.97915559e-05]\n",
      " [-5.07320044e-03  6.33923708e-04  3.50109537e-01 ...  3.35149843e-05\n",
      "  -1.69674313e-05 -1.41927595e-04]\n",
      " [ 9.87462614e-04 -1.51536675e-03 -1.30294343e-02 ...  2.03885182e-04\n",
      "  -2.94564359e-04  2.87271283e-04]\n",
      " ...\n",
      " [ 7.44804045e-06  3.82327366e-05  1.06757401e-04 ...  1.03589776e-02\n",
      "  -6.90604173e-03  7.97346816e-03]\n",
      " [-3.81727851e-04 -3.66072720e-04  1.62982713e-03 ...  3.12063336e-03\n",
      "   5.27417774e-03  2.55176477e-03]\n",
      " [-1.07345488e-04  8.09362057e-06  5.44561849e-04 ...  7.74459865e-04\n",
      "   1.61464035e-03  1.03226381e-03]]\n",
      "\n",
      " Eigen Values \n",
      "%s [1.15422544e+05 8.97775583e+04 4.78046841e+03 3.57188554e+03\n",
      " 2.53279714e+03 1.96166757e+03 1.81784313e+03 8.50975431e+02\n",
      " 8.10451380e+02 7.57573098e+02 6.46093009e+02 5.76828492e+02\n",
      " 5.84560620e+02 5.86033924e+02 4.49140636e+02 4.18048443e+02\n",
      " 3.78008417e+02 3.23614705e+02 3.15696678e+02 2.82814730e+02\n",
      " 2.58664860e+02 8.81230801e+01 7.86017652e+01 7.39932005e+01\n",
      " 6.66264256e+01 5.07264662e+01 4.41575651e+01 4.09040561e+01\n",
      " 3.55349414e+01 2.79782828e+01 2.58197579e+01 2.34426739e+01\n",
      " 2.10245988e+01 1.86562111e+01 1.44189137e+01 1.21032705e+01\n",
      " 1.12819092e+01 1.04734006e+01 1.00971365e+01 8.23770404e+00\n",
      " 7.63341294e+00 7.08568338e+00 6.04757980e+00 5.84738476e+00\n",
      " 5.38847183e+00 5.17372174e+00 4.91331045e+00 4.61960878e+00\n",
      " 4.34373359e+00 4.13536689e+00 3.95068600e+00 3.31984766e+00\n",
      " 3.08909452e+00 2.87805443e+00 2.51643897e+00 2.44942902e+00\n",
      " 1.92549313e+00 1.77657074e+00 1.72430061e+00 1.47813145e+00\n",
      " 1.34243429e+00 1.26619762e+00 9.05156605e-01 8.82667439e-01\n",
      " 7.83999602e-01 7.37601528e-01 6.49996414e-01 5.95293630e-01\n",
      " 5.68007513e-01 5.19049143e-01 4.71104861e-01 4.29368138e-01\n",
      " 3.90372324e-01 2.51664007e-01 2.14071095e-01 1.61112653e-01\n",
      " 1.43854627e-01 7.96821049e-02 7.49291674e-02 7.06453382e-02\n",
      " 6.57285113e-02 6.16124869e-02 5.97121533e-02 5.60578287e-02\n",
      " 4.64743859e-02 3.18317960e-02 2.72738857e-02 1.72813121e-02\n",
      " 1.58915347e-02 1.45497279e-02 1.36376479e-02 2.36345456e-03\n",
      " 2.81234246e-03 8.85367935e-03 8.71156902e-03 4.02743345e-03\n",
      " 4.98965975e-03 5.17401662e-03 5.69965533e-03 6.07719542e-03\n",
      " 6.58878263e-03 6.38582135e-03]\n"
     ]
    }
   ],
   "source": [
    "# Step 2- Get eigen values and eigen vector\n",
    "eig_vals, eig_vecs = np.linalg.eig(covMatrix)\n",
    "print('Eigen Vectors \\n%s', eig_vecs)\n",
    "print('\\n Eigen Values \\n%s', eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0512d63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Variance Explained [ 50.65574377  90.05661657  92.15463117  93.72223237  94.83380658\n",
      "  95.6947279   96.49252868  96.86599812  97.22168269  97.55416045\n",
      "  97.83771269  98.09490668  98.35145408  98.60460805  98.80172338\n",
      "  98.98519322  99.1510906   99.29311609  99.43166658  99.55578609\n",
      "  99.6693069   99.70798167  99.7424778   99.77495136  99.80419184\n",
      "  99.82645428  99.84583381  99.86378546  99.87938075  99.89165964\n",
      "  99.90299122  99.91327956  99.92250667  99.93069436  99.93702242\n",
      "  99.94233421  99.94728552  99.95188201  99.95631336  99.95992866\n",
      "  99.96327875  99.96638846  99.96904258  99.97160883  99.97397368\n",
      "  99.97624428  99.9784006   99.98042802  99.98233436  99.98414926\n",
      "  99.9858831   99.98734009  99.98869581  99.98995891  99.9910633\n",
      "  99.99213829  99.99298334  99.99376302  99.99451977  99.99516848\n",
      "  99.99575764  99.99631334  99.99671059  99.99709796  99.99744204\n",
      "  99.99776575  99.99805102  99.99831227  99.99856156  99.99878935\n",
      "  99.99899611  99.99918455  99.99935587  99.99946632  99.99956027\n",
      "  99.99963098  99.99969411  99.99972908  99.99976196  99.99979297\n",
      "  99.99982182  99.99984886  99.99987506  99.99989966  99.99992006\n",
      "  99.99993403  99.999946    99.99995358  99.99996056  99.99996694\n",
      "  99.99997293  99.99997681  99.99998064  99.99998353  99.99998633\n",
      "  99.999989    99.9999915   99.99999377  99.99999596  99.99999773\n",
      "  99.99999896 100.        ]\n"
     ]
    }
   ],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cumulative Variance Explained\", cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c1a2cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.38504317e+02,  7.06217915e+00, -2.03790078e+02, ...,\n",
       "         1.55512947e+02,  1.83730761e+02,  2.74895244e+02],\n",
       "       [ 3.24992982e+01,  3.71294834e+02, -5.18945239e+02, ...,\n",
       "         1.85955921e+02, -1.03563688e+02, -1.69392721e+02],\n",
       "       [-3.83710144e+01,  3.58237818e+01, -9.50651284e+01, ...,\n",
       "        -1.28746545e+01,  4.93299218e+01,  2.68968263e+01],\n",
       "       ...,\n",
       "       [ 9.28560023e-03,  3.86667758e-03, -3.23105680e-02, ...,\n",
       "         2.87087815e-02, -3.49724293e-02,  3.28477955e-02],\n",
       "       [-1.02565111e-01, -4.12652272e-02, -4.35679474e-02, ...,\n",
       "        -3.32815649e-02, -5.41466999e-03,  2.02222828e-02],\n",
       "       [ 1.65936727e-02, -5.59188830e-03,  4.78022220e-03, ...,\n",
       "        -2.27541273e-02, -5.42115936e-02,  2.58578435e-02]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "data_reduced = pca.fit_transform(XScaled)\n",
    "data_reduced.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c922d8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.53389718e-03, -5.07320044e-03,  9.87462614e-04, ...,\n",
       "         7.44804045e-06, -3.81727851e-04, -1.07345488e-04],\n",
       "       [-1.30888965e-02,  6.33923708e-04, -1.51536675e-03, ...,\n",
       "         3.82327366e-05, -3.66072720e-04,  8.09362057e-06],\n",
       "       [ 7.26789867e-01, -3.50109537e-01,  1.30294343e-02, ...,\n",
       "        -1.06757401e-04, -1.62982713e-03, -5.44561849e-04],\n",
       "       ...,\n",
       "       [ 8.27547768e-05, -8.59883619e-06,  1.06330711e-04, ...,\n",
       "        -6.14633646e-03, -2.61598785e-03, -1.81743261e-04],\n",
       "       [ 3.47209140e-05,  2.59585495e-05, -1.99536019e-04, ...,\n",
       "        -2.45032006e-02, -3.61976819e-03,  5.08162563e-04],\n",
       "       [-5.43152445e-05,  2.52057474e-05, -2.68700336e-05, ...,\n",
       "         4.73410924e-03,  1.85767571e-03, -1.33943462e-04]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26c3845d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10 , 5))\n",
    "plt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
    "plt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\n",
    "plt. axhline(y=90,linewidth=2, color='g')\n",
    "plt.axvline(x=2, linewidth = 3, color ='b')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16f2e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the PCA we could reduce the features to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "453e7101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_rd = PCA(n_components=2)\n",
    "pca_rd.fit(XScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "abf97a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XScaled_pca.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7020e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after dimensionality reduction: (2926, 2)\n"
     ]
    }
   ],
   "source": [
    "XScaled_pca = pd.DataFrame(pca_rd.transform(data_reduced))\n",
    "\n",
    "print(\"shape after dimensionality reduction:\", XScaled_pca.shape)\n",
    "\n",
    "X_pca_train,X_pca_test,y_train,y_test= train_test_split(XScaled_pca,y_sm, test_size =0.3,stratify = y_sm,random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2532f7",
   "metadata": {},
   "source": [
    "E. Display and explain the classification report in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14a76658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2048, 2), (878, 2), (2048,), (878,), (2926, 102))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_pca_train.shape, X_pca_test.shape,y_train.shape,y_test.shape,X_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b92cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5339d575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0         Random Forest       0.986333             1.0      0.993   \n",
       "1  Random forest(tuned)       0.989749             1.0      0.993   \n",
       "2    Random forest(pca)       0.611617             1.0      0.604   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  \n",
       "1               0.986               0.993                0.007  \n",
       "2               0.649               0.574                0.426  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_pca = RandomForestClassifier( bootstrap = False,\n",
    "  max_depth = 30,\n",
    "  max_features =  'auto' ,\n",
    "  min_samples_leaf = 1,\n",
    "  min_samples_split = 2,\n",
    "  n_estimators = 200, random_state= 1)\n",
    "\n",
    "rfc_pca.fit(X_pca_train, y_train)\n",
    "\n",
    "pred = rfc_pca.predict(X_pca_test)\n",
    "\n",
    "tn, fp, fn, tp =confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "forest_score = rfc_pca.score(X_pca_test, y_test)\n",
    "forest_recall = round(tp/(tp+fn), 3)   # Of all the signal , how many were truly recognised to have 'Pass'\n",
    "forest_precision = round(tp/(tp+fp), 3)  # Of all the signal predicted , how many truly predicted as 'Pass'\n",
    "forest_specificity =round(tn/(tn+fp),3) # Of all the signals, how many were recognised as truly 'Fail'\n",
    "\n",
    "train_pred = rfc_pca.predict(X_pca_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "\n",
    "result.loc[2] = ['Random forest(pca)', forest_score,train_acc, forest_precision, forest_recall, forest_specificity, 1 - forest_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0beaaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Pass       0.62      0.57      0.60       439\n",
      "        Fail       0.60      0.65      0.63       439\n",
      "\n",
      "    accuracy                           0.61       878\n",
      "   macro avg       0.61      0.61      0.61       878\n",
      "weighted avg       0.61      0.61      0.61       878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Pass', 'Fail']\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d24b4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying pca and tuning did not work for this model. pca reduced 102 columns to 2 columns but couldn't explain the complete model completely.\n",
    "# it looks like our Baseline Random Forest model did the best, with the highest f1 score,accuracy, recall and precision. of 99. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344fabaf",
   "metadata": {},
   "source": [
    "5F. Apply the above steps for all possible models that you have learnt so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef5ec0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0         Random Forest       0.986333        1.000000      0.993   \n",
       "1  Random forest(tuned)       0.989749        1.000000      0.993   \n",
       "2    Random forest(pca)       0.611617        1.000000      0.604   \n",
       "3   Logistic Regression       0.494305        0.590332      0.519   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  \n",
       "1               0.986               0.993                0.007  \n",
       "2               0.649               0.574                0.426  \n",
       "3               0.495               0.469                0.531  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "pred = logreg.predict(X_test_scaled)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,pred).ravel()\n",
    "\n",
    "lr_score = logreg.score(X_test_scaled, y_test)\n",
    "lr_recall = round(tp/(tp+fn), 3)  # Of all the signal , how many were truly recognised to have 'Pass' \n",
    "lr_precision = round(tp/(tp+fp), 3) # Of all the signal predicted , how many truly predicted as 'Pass'\n",
    "lr_specificity =round(tn/(tn+fp),3)   # Of all the signals, how many were recognised as truly 'Fail' \n",
    "\n",
    "\n",
    "train_pred = logreg.predict(x_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result.loc[3] = ['Logistic Regression', lr_score, train_acc,lr_recall, lr_precision, lr_specificity, 1 - lr_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0de652a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression(pca)</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.575195</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0             Random Forest       0.986333        1.000000      0.993   \n",
       "1      Random forest(tuned)       0.989749        1.000000      0.993   \n",
       "2        Random forest(pca)       0.611617        1.000000      0.604   \n",
       "3       Logistic Regression       0.494305        0.590332      0.519   \n",
       "4  Logistic Regression(pca)       0.555809        0.575195      0.538   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  \n",
       "1               0.986               0.993                0.007  \n",
       "2               0.649               0.574                0.426  \n",
       "3               0.495               0.469                0.531  \n",
       "4               0.558               0.574                0.426  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying on pca components\n",
    "logreg_pca = LogisticRegression(solver='liblinear', random_state=0)\n",
    "\n",
    "logreg_pca.fit(X_pca_train, y_train)\n",
    "\n",
    "pred = logreg_pca.predict(X_pca_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,pred).ravel()\n",
    "\n",
    "lr_score = logreg_pca.score(X_pca_test, y_test)\n",
    "lr_recall = round(tp/(tp+fn), 3)   # Of all the signal , how many were truly recognised to have 'Pass'\n",
    "lr_precision = round(tp/(tp+fp), 3) # Of all the signal predicted , how many truly predicted as 'Pass'\n",
    "lr_specificity =round(tn/(tn+fp),3)  # Of all the signals, how many were recognised as truly 'Fail'  \n",
    "\n",
    "train_pred = logreg_pca.predict(X_pca_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "\n",
    "result.loc[4] = ['Logistic Regression(pca)', lr_score,train_acc, lr_recall, lr_precision, lr_specificity, 1 - lr_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "783fab3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression(pca)</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.575195</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.523918</td>\n",
       "      <td>0.589355</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0             Random Forest       0.986333        1.000000      0.993   \n",
       "1      Random forest(tuned)       0.989749        1.000000      0.993   \n",
       "2        Random forest(pca)       0.611617        1.000000      0.604   \n",
       "3       Logistic Regression       0.494305        0.590332      0.519   \n",
       "4  Logistic Regression(pca)       0.555809        0.575195      0.538   \n",
       "5             Decision Tree       0.523918        0.589355      0.516   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  \n",
       "1               0.986               0.993                0.007  \n",
       "2               0.649               0.574                0.426  \n",
       "3               0.495               0.469                0.531  \n",
       "4               0.558               0.574                0.426  \n",
       "5               0.774               0.273                0.727  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth =4,random_state=1)\n",
    "\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "pred = dt.predict(X_test_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "dt_score = dt.score(X_test_scaled, y_test)\n",
    "dt_recall = round(tp/(tp+fn), 3)   # Of all the signal , how many were truly recognised to have 'Pass'\n",
    "dt_precision = round(tp/(tp+fp), 3) # Of all the signal predicted , how many truly predicted as 'Pass'\n",
    "dt_specificity =round(tn/(tn+fp),3)  # Of all the signals, how many were recognised as truly 'Fail' \n",
    "\n",
    "train_pred = dt.predict(x_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "\n",
    "result.loc[5] = ['Decision Tree', dt_score,train_acc, dt_precision, dt_recall, dt_specificity, 1-dt_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "440f122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_dt = dt.predict(x_train)\n",
    "test_pred_dt = dt.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2b082ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying decision tree on PCA components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "11fe90ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression(pca)</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.575195</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.523918</td>\n",
       "      <td>0.589355</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree(pca)</td>\n",
       "      <td>0.576310</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0             Random Forest       0.986333        1.000000      0.993   \n",
       "1      Random forest(tuned)       0.989749        1.000000      0.993   \n",
       "2        Random forest(pca)       0.611617        1.000000      0.604   \n",
       "3       Logistic Regression       0.494305        0.590332      0.519   \n",
       "4  Logistic Regression(pca)       0.555809        0.575195      0.538   \n",
       "5             Decision Tree       0.523918        0.589355      0.516   \n",
       "6        Decision Tree(pca)       0.576310        0.612305      0.562   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  \n",
       "1               0.986               0.993                0.007  \n",
       "2               0.649               0.574                0.426  \n",
       "3               0.495               0.469                0.531  \n",
       "4               0.558               0.574                0.426  \n",
       "5               0.774               0.273                0.727  \n",
       "6               0.690               0.462                0.538  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_pca= DecisionTreeClassifier(max_depth =4,random_state=1)\n",
    "\n",
    "dt_pca.fit(X_pca_train, y_train)\n",
    "\n",
    "pred = dt_pca.predict(X_pca_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "dt_score = dt_pca.score(X_pca_test, y_test)\n",
    "dt_recall = round(tp/(tp+fn), 3)   # Of all the signal , how many were truly recognised to have 'Pass'\n",
    "dt_precision = round(tp/(tp+fp), 3) # Of all the signal predicted , how many truly predicted as 'Pass'\n",
    "dt_specificity =round(tn/(tn+fp),3)  # Of all the signals, how many were recognised as truly 'Fail' \n",
    "\n",
    "train_pred = dt_pca.predict(X_pca_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "\n",
    "result.loc[6] = ['Decision Tree(pca)', dt_score, train_acc,dt_precision, dt_recall, dt_specificity, 1-dt_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "49e2c536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression(pca)</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.575195</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.523918</td>\n",
       "      <td>0.589355</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree(pca)</td>\n",
       "      <td>0.576310</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.506834</td>\n",
       "      <td>0.969238</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0             Random Forest       0.986333        1.000000      0.993   \n",
       "1      Random forest(tuned)       0.989749        1.000000      0.993   \n",
       "2        Random forest(pca)       0.611617        1.000000      0.604   \n",
       "3       Logistic Regression       0.494305        0.590332      0.519   \n",
       "4  Logistic Regression(pca)       0.555809        0.575195      0.538   \n",
       "5             Decision Tree       0.523918        0.589355      0.516   \n",
       "6        Decision Tree(pca)       0.576310        0.612305      0.562   \n",
       "7                       SVM       0.506834        0.969238      0.519   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  \n",
       "1               0.986               0.993                0.007  \n",
       "2               0.649               0.574                0.426  \n",
       "3               0.495               0.469                0.531  \n",
       "4               0.558               0.574                0.426  \n",
       "5               0.774               0.273                0.727  \n",
       "6               0.690               0.462                0.538  \n",
       "7               0.185               0.829                0.171  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(C=1, gamma=1, kernel='rbf')\n",
    "\n",
    "svm.fit(x_train, y_train)\n",
    "\n",
    "pred = svm.predict(X_test_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "svm_score = svm.score(X_test_scaled, y_test)\n",
    "svm_recall = round(tp/(tp+fn), 3)   # Of all the signal , how many were truly recognised to have 'Pass'\n",
    "svm_precision = round(tp/(tp+fp), 3)  # Of all the signal predicted , how many truly predicted as 'Pass'\n",
    "svm_specificity =round(tn/(tn+fp),3) # Of all the signals, how many were recognised as truly 'Fail'\n",
    "\n",
    "train_pred = svm.predict(x_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "\n",
    "result.loc[7] = ['SVM', svm_score,train_acc, svm_precision, svm_recall, svm_specificity, 1 - svm_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "58d4bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_svm = svm.predict(x_train)\n",
    "test_pred_svm = svm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5f764b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression(pca)</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.575195</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.523918</td>\n",
       "      <td>0.589355</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree(pca)</td>\n",
       "      <td>0.576310</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.506834</td>\n",
       "      <td>0.969238</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVM(pca)</td>\n",
       "      <td>0.620729</td>\n",
       "      <td>0.684570</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0             Random Forest       0.986333        1.000000      0.993   \n",
       "1      Random forest(tuned)       0.989749        1.000000      0.993   \n",
       "2        Random forest(pca)       0.611617        1.000000      0.604   \n",
       "3       Logistic Regression       0.494305        0.590332      0.519   \n",
       "4  Logistic Regression(pca)       0.555809        0.575195      0.538   \n",
       "5             Decision Tree       0.523918        0.589355      0.516   \n",
       "6        Decision Tree(pca)       0.576310        0.612305      0.562   \n",
       "7                       SVM       0.506834        0.969238      0.519   \n",
       "8                  SVM(pca)       0.620729        0.684570      0.616   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  \n",
       "1               0.986               0.993                0.007  \n",
       "2               0.649               0.574                0.426  \n",
       "3               0.495               0.469                0.531  \n",
       "4               0.558               0.574                0.426  \n",
       "5               0.774               0.273                0.727  \n",
       "6               0.690               0.462                0.538  \n",
       "7               0.185               0.829                0.171  \n",
       "8               0.640               0.601                0.399  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svm is not good for this data.lets try with pca components\n",
    "svm_pca = SVC(C=1, gamma=1, kernel='rbf')\n",
    "\n",
    "svm_pca.fit(X_pca_train, y_train)\n",
    "\n",
    "pred = svm_pca.predict(X_pca_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "svm_score = svm_pca.score(X_pca_test, y_test)\n",
    "svm_recall = round(tp/(tp+fn), 3)   # Of all the signal , how many were truly recognised to have 'Pass'\n",
    "svm_precision = round(tp/(tp+fp), 3)  # Of all the signal predicted , how many truly predicted as 'Pass'\n",
    "svm_specificity =round(tn/(tn+fp),3) # Of all the signals, how many were recognised as truly 'Fail'\n",
    "\n",
    "train_pred = svm_pca.predict(X_pca_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "\n",
    "result.loc[8] = ['SVM(pca)', svm_score,train_acc, svm_precision, svm_recall, svm_specificity, 1 - svm_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "beb4bd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression(pca)</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.575195</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.523918</td>\n",
       "      <td>0.589355</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree(pca)</td>\n",
       "      <td>0.576310</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.506834</td>\n",
       "      <td>0.969238</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVM(pca)</td>\n",
       "      <td>0.620729</td>\n",
       "      <td>0.684570</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.490888</td>\n",
       "      <td>0.751465</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0             Random Forest       0.986333        1.000000      0.993   \n",
       "1      Random forest(tuned)       0.989749        1.000000      0.993   \n",
       "2        Random forest(pca)       0.611617        1.000000      0.604   \n",
       "3       Logistic Regression       0.494305        0.590332      0.519   \n",
       "4  Logistic Regression(pca)       0.555809        0.575195      0.538   \n",
       "5             Decision Tree       0.523918        0.589355      0.516   \n",
       "6        Decision Tree(pca)       0.576310        0.612305      0.562   \n",
       "7                       SVM       0.506834        0.969238      0.519   \n",
       "8                  SVM(pca)       0.620729        0.684570      0.616   \n",
       "9                       knn       0.490888        0.751465      0.491   \n",
       "\n",
       "   True positive rate  True negative rate  False positive rate  \n",
       "0               0.979               0.993                0.007  \n",
       "1               0.986               0.993                0.007  \n",
       "2               0.649               0.574                0.426  \n",
       "3               0.495               0.469                0.531  \n",
       "4               0.558               0.574                0.426  \n",
       "5               0.774               0.273                0.727  \n",
       "6               0.690               0.462                0.538  \n",
       "7               0.185               0.829                0.171  \n",
       "8               0.640               0.601                0.399  \n",
       "9               0.524               0.458                0.542  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors= 3, n_jobs=1000)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "pred = knn.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "knn_score = knn.score(X_test_scaled, y_test)\n",
    "knn_recall = round(tp/(tp+fn), 3)   # Of all the signal , how many were truly recognised to have 'Pass' \n",
    "knn_precision = round(tp/(tp+fp), 3) # Of all the signal predicted , how many truly predicted as 'Pass' \n",
    "knn_specificity =round(tn/(tn+fp),3) # Of all the signals, how many were recognised as truly 'Fail'\n",
    "\n",
    "train_pred = knn.predict(x_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "\n",
    "result.loc[9] = ['knn', knn_score,train_acc, knn_precision, knn_recall, knn_specificity, 1 - knn_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e7dd5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_knn = knn.predict(x_train)\n",
    "test_pred_knn = knn.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "19f04103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>True positive rate</th>\n",
       "      <th>True negative rate</th>\n",
       "      <th>False positive rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression(pca)</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.575195</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.523918</td>\n",
       "      <td>0.589355</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree(pca)</td>\n",
       "      <td>0.576310</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.506834</td>\n",
       "      <td>0.969238</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVM(pca)</td>\n",
       "      <td>0.620729</td>\n",
       "      <td>0.684570</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.490888</td>\n",
       "      <td>0.751465</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>knn(pca)</td>\n",
       "      <td>0.596811</td>\n",
       "      <td>0.806152</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Test Accuracy  Train Accuracy  Precision  \\\n",
       "0              Random Forest       0.986333        1.000000      0.993   \n",
       "1       Random forest(tuned)       0.989749        1.000000      0.993   \n",
       "2         Random forest(pca)       0.611617        1.000000      0.604   \n",
       "3        Logistic Regression       0.494305        0.590332      0.519   \n",
       "4   Logistic Regression(pca)       0.555809        0.575195      0.538   \n",
       "5              Decision Tree       0.523918        0.589355      0.516   \n",
       "6         Decision Tree(pca)       0.576310        0.612305      0.562   \n",
       "7                        SVM       0.506834        0.969238      0.519   \n",
       "8                   SVM(pca)       0.620729        0.684570      0.616   \n",
       "9                        knn       0.490888        0.751465      0.491   \n",
       "10                  knn(pca)       0.596811        0.806152      0.599   \n",
       "\n",
       "    True positive rate  True negative rate  False positive rate  \n",
       "0                0.979               0.993                0.007  \n",
       "1                0.986               0.993                0.007  \n",
       "2                0.649               0.574                0.426  \n",
       "3                0.495               0.469                0.531  \n",
       "4                0.558               0.574                0.426  \n",
       "5                0.774               0.273                0.727  \n",
       "6                0.690               0.462                0.538  \n",
       "7                0.185               0.829                0.171  \n",
       "8                0.640               0.601                0.399  \n",
       "9                0.524               0.458                0.542  \n",
       "10               0.585               0.608                0.392  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors= 3, n_jobs=1000)\n",
    "\n",
    "knn_pca.fit(X_pca_train, y_train)\n",
    "\n",
    "\n",
    "pred = knn_pca.predict(X_pca_test)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "knn_score = knn_pca.score(X_pca_test, y_test)\n",
    "knn_recall = round(tp/(tp+fn), 3)    # Of all the signal , how many were truly recognised to have 'Pass'\n",
    "knn_precision = round(tp/(tp+fp), 3) # Of all the signal predicted , how many truly predicted as 'Pass' \n",
    "knn_specificity =round(tn/(tn+fp),3) # Of all the signals, how many were recognised as truly 'Fail'\n",
    "\n",
    "train_pred = knn_pca.predict(X_pca_train)\n",
    "train_acc = accuracy_score(y_train,train_pred)\n",
    "\n",
    "\n",
    "result.loc[10] = ['knn(pca)', knn_score,train_acc, knn_precision, knn_recall, knn_specificity, 1 - knn_specificity]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e5b7a",
   "metadata": {},
   "source": [
    "6.A. Post Training and Conclusion: \n",
    "A. Display and compare all the models designed with their train and test accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72809f0f",
   "metadata": {},
   "source": [
    "CONCLUSION From all these models it is seen that performance increase happens when applied on pca components than their respective base models.However  Random Forest tuned Model outperformed everything else. with only .005 % of signals getting classified as false positive that is Those 'Fail' signals were falsely classified as 'Fail'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29891a",
   "metadata": {},
   "source": [
    "6A. Display and compare all the models designed with their train and test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "84c4d91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random forest(tuned)</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random forest(pca)</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.590332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression(pca)</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.575195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.523918</td>\n",
       "      <td>0.589355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree(pca)</td>\n",
       "      <td>0.576310</td>\n",
       "      <td>0.612305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.506834</td>\n",
       "      <td>0.969238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVM(pca)</td>\n",
       "      <td>0.620729</td>\n",
       "      <td>0.684570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.490888</td>\n",
       "      <td>0.751465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>knn(pca)</td>\n",
       "      <td>0.596811</td>\n",
       "      <td>0.806152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Test Accuracy  Train Accuracy\n",
       "0              Random Forest       0.986333        1.000000\n",
       "1       Random forest(tuned)       0.989749        1.000000\n",
       "2         Random forest(pca)       0.611617        1.000000\n",
       "3        Logistic Regression       0.494305        0.590332\n",
       "4   Logistic Regression(pca)       0.555809        0.575195\n",
       "5              Decision Tree       0.523918        0.589355\n",
       "6         Decision Tree(pca)       0.576310        0.612305\n",
       "7                        SVM       0.506834        0.969238\n",
       "8                   SVM(pca)       0.620729        0.684570\n",
       "9                        knn       0.490888        0.751465\n",
       "10                  knn(pca)       0.596811        0.806152"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[['Model','Test Accuracy','Train Accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66ecd0",
   "metadata": {},
   "source": [
    "6B. Select the final best trained model along with your detailed comments for selecting this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc9547",
   "metadata": {},
   "source": [
    "From the above dataset of result it can be seen that the random forest outperformed rest of all the models and when tuned it went to give better accuracy.Random Forest is an ensemble of classification algorithm widely used in much application especially with larger datasets because of its outstanding features like Variable Importance measure, OOB error detection, Proximity among the feature and handling of imbalanceddatasets.This random selection of the predictor variables results in less correlation among the trees and has a lower error rate . To predict target value for new data instance, the new observation is  fed to  all  classification trees  in  the Random  Forest.  The numbers of  prediction for  a class performed  by each  of the classification  trees  are  counted.  Then,  the  class  with  the maximum number of votes is returned as the class label  for the new data instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba7c4e",
   "metadata": {},
   "source": [
    "6C. Pickle the selected model for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2759099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "Pickled = pickle.dumps(forest_tuned) \n",
    "    \n",
    "Pickled_Load = pickle.loads(Pickled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47af553",
   "metadata": {},
   "source": [
    "6D.Write your conclusion on the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "From all these models it is seen that performance increase happens when applied on pca components than their respective base models.However Random Forest tuned Model outperformed everything else. with only .005 % of signals getting classified as false positive that is Those 'Fail' signals were falsely classified as 'Fail'.\n",
    "The dataset which had hundreds of features were put down to 102 features and random forest classifier could almost correctly classify the signals as \"pass\"\" or \"Fail\", with very few misclassification.\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f78d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
